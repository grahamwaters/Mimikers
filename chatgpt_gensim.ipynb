{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httplib\n",
    "import gensim\n",
    "import json\n",
    "\n",
    "# Load categories from JSON file\n",
    "with open(\"categories.json\", \"r\") as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "# Shuffle categories and select the first 20\n",
    "random.shuffle(categories)\n",
    "categories = categories[:20]\n",
    "\n",
    "# Build URL for Toolforge API request\n",
    "URL = \"https://randomincategory.toolforge.org/Random_page_in_category?\"\n",
    "for i, cat in enumerate(categories):\n",
    "    URL += f\"&category{i}={urllib.parse.quote(cat.lower())}\"\n",
    "URL += \"&server=en.wikipedia.org&cmnamespace=0&cmtype=page&returntype=\"\n",
    "\n",
    "# Load URLs from CSV file\n",
    "urls_df = pd.read_csv(\"./peoplelinks.csv\", error_bad_lines=False)\n",
    "urls = [url for url in urls_df.values if \"wikipedia\" in url]\n",
    "\n",
    "# Load word2vec model and create a dictionary for caching results\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "cached_results = {}\n",
    "\n",
    "# Iterate over URLs and summarize each page\n",
    "for url in tqdm(urls):\n",
    "    # Check if the URL is in the cache\n",
    "    if url in cached_results:\n",
    "        summary = cached_results[url]\n",
    "    else:\n",
    "        # Use httplib to make a GET request to the URL\n",
    "        conn = httplib.HTTPSConnection(url)\n",
    "        conn.request(\"GET\", \"/\")\n",
    "        response = conn.getresponse()\n",
    "        page_html = response.read()\n",
    "\n",
    "        # Use BeautifulSoup to parse the HTML and extract the page text\n",
    "        soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "        page_text = soup.get_text()\n",
    "\n",
    "        # Use Gensim's word2vec model to summarize the text\n",
    "        summary = model.summarize(page_text, ratio=0.1)\n",
    "\n",
    "        # Cache the result\n",
    "        cached_results[url] = summary\n",
    "\n",
    "# Save the cache to a JSON file\n",
    "with open(\"cached_results.json\", \"w\") as f:\n",
    "    json.dump(cached_results, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
