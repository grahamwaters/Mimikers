{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_wgZ8DsdcYz",
        "outputId": "51680fd1-495e-40fa-b5bc-38c2fdc650b5"
      },
      "outputs": [],
      "source": [
        "# !pip install requests\n",
        "# !pip install beautifulsoup4\n",
        "# !pip install wikipedia\n",
        "# !pip install PyDictionary\n",
        "# !pip install pytrends\n",
        "# !pip install urbandictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 10 trending searches.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>exploreQuery</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wordle</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Election results</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Betty White</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Queen Elizabeth</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bob Saget</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Ukraine</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mega Millions</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Powerball numbers</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Anne Heche</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Jeffrey Dahmer</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               title exploreQuery\n",
              "0             Wordle             \n",
              "1   Election results             \n",
              "2        Betty White             \n",
              "3    Queen Elizabeth             \n",
              "4          Bob Saget             \n",
              "5            Ukraine             \n",
              "6      Mega Millions             \n",
              "7  Powerball numbers             \n",
              "8         Anne Heche             \n",
              "9     Jeffrey Dahmer             "
            ]
          },
          "execution_count": 224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pytrends\n",
        "from pytrends.request import TrendReq\n",
        "import wikipedia\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import PyDictionary\n",
        "from PyDictionary import PyDictionary\n",
        "#import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "dir(pytrends)\n",
        "pytrends = TrendReq(hl='en-US', tz=360) # create a pytrends object\n",
        "#pytrends.trending_searches(pn='united_states') # trending searches in real time for United States\n",
        "# create a dictionary of the top 1000 trending searches for this year in the United States (as of 2022)\n",
        "trending_searches_dict = pytrends.top_charts(\n",
        "    date='2022',    # specifies the time period as the last year\n",
        "    geo='US',            # specifies the country as the United States\n",
        ")\n",
        "real_time_trending_searches_dict = pytrends.trending_searches(pn='united_states') # trending searches in real time for United States\n",
        "\n",
        "\n",
        "\n",
        "# show how many trending searches there are\n",
        "print(f'Retrieved {len(trending_searches_dict)} trending searches.')\n",
        "# show the top 10 trending searches in real time for United States\n",
        "trending_searches_dict.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urbandictionary as ud\n",
        "import re\n",
        "# adding import alias recommended\n",
        "bad_patterns = [r'sex*', r'porn*',r'fuck*',r'-ass*','ass','shit',r'damn*',r'ass|asse*',r'cock*',r'whor*',r'nigg*',r'slut*','blowjob',r'fagg*',r'boob|boob*', r'bitch*', r'bastard*',' ho |hoe',r'breast*|jugs', r'cunt*', r'puss*', r'dick*', 'naked', r'nud*', r'masterb*',r'mastu',r'nipple*',r'penis|penal|peni*','god','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','ejaculate','jew*','islamic','atheist',r'rapist*|rape*',r'pedo*','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','gay',r'tit*|titt*','god','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','islamic','atheist','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','yahwey','yeshua',r'israel*',r'sex*', r'porn*',r'fuck*',r'-ass*','ass','shit',r'damn*',r'ass|asse*',r'cock*',r'whor*',r'nigg*',r'slut*','blowjob',r'fagg*',r'boob|boob*', r'breast*|jugs', r'cunt*', r'puss*', r'dick*', 'naked', r'nud*', r'nipple*',r'penis|penal|peni*','god','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','islamic','atheist','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','gay',r'tit*|titt*', 'fellatio', 'fuck', 'nigger','lynch',r'erotic*']\n",
        "\n",
        "# to make the game more fun we can replace words with less inflamatory ones.\n",
        "\n",
        "\n",
        "def check_for_badwords(definition, bad_patterns):\n",
        "    definition = definition.lower()\n",
        "    # if any of the buzzwords are found return true else false\n",
        "    if any(re.search(r'\\b' + word + r'\\b', definition) for word in bad_patterns):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def check_for_good_patterns(definition, title):\n",
        "    # check both title and definition for good patterns\n",
        "    good_patterns = [r'\\b\\w+phobia\\.?\\b', r'\\bslang\\b', r'\\bacronymn\\b', r'\\bmeme\\b']\n",
        "\n",
        "\n",
        "    if any(re.match(r'\\b' + word + r'\\b', definition) for word in good_patterns):\n",
        "        print(f'Found a good pattern in the definition: {definition}')\n",
        "        return True\n",
        "    elif any(re.match(r'\\b' + word + r'\\b', title) for word in good_patterns):\n",
        "        print(f'Found a good pattern in the title: {title}')\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def remove_undesireable_sentences(definition, title, bad_patterns):\n",
        "    # convert the definition and title to lowercase\n",
        "    definition = definition.lower()\n",
        "    title = title.lower()\n",
        "\n",
        "    # split the definition into sentences\n",
        "    sentences = definition.split('.')\n",
        "\n",
        "    # split the title into words\n",
        "    title_words = title.split()\n",
        "\n",
        "    # initialize a list to store the acceptable sentences\n",
        "    acceptable_sentences = []\n",
        "\n",
        "    # iterate over the sentences in the definition\n",
        "    for sentence in sentences:\n",
        "        # split the sentence into words\n",
        "        sentence_words = sentence.split()\n",
        "\n",
        "        # check if any of the words in the sentence are also in the title\n",
        "        if not any(word in title_words for word in sentence_words):\n",
        "            # if not, add the sentence to the list of acceptable sentences\n",
        "            acceptable_sentences.append(sentence)\n",
        "\n",
        "    # remove sentences that are not in English\n",
        "    definition = re.sub(r'[^\\x00-\\x7f]',r'', '.'.join(acceptable_sentences))\n",
        "    # remove sentences that contain a regex match to any word in the buzzwords list\n",
        "    definition = re.sub(r'|'.join(map(re.escape, bad_patterns)), '', definition)\n",
        "    return definition\n",
        "\n",
        "\n",
        "def unpack_definitions(definition):\n",
        "    # remove the brackets and clean up the definitions\n",
        "    # with regex\n",
        "    definition = definition.replace(\"[\",\"\")\n",
        "    definition = definition.replace(\"]\",\"\")\n",
        "    definition = definition.replace(\"'\",\"\")\n",
        "    definition = definition.replace('\"',\"\")\n",
        "    definition = definition.replace(\"(\",\"\")\n",
        "    definition = definition.replace(\")\",\"\")\n",
        "\n",
        "    # remove double spaces\n",
        "    definition = definition.replace('  ',' ')\n",
        "\n",
        "    return definition\n",
        "\n",
        "import wikipedia\n",
        "\n",
        "def get_page_length(phrase):\n",
        "  # search for pages on Wikipedia that match the given phrase\n",
        "    try:\n",
        "        pages = wikipedia.search(phrase)\n",
        "\n",
        "        # retrieve the first page from the search results\n",
        "        page = wikipedia.page(pages[0])\n",
        "        print(f'Found the page for {page.title}', end='')\n",
        "        if phrase.lower() != page.title.lower():\n",
        "            print(' ... nevermind... not the right page.')\n",
        "            return 0\n",
        "        # return the length of the page\n",
        "        return len(page.content)\n",
        "    except Exception as e:\n",
        "            return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added aint no thang but a chicken wang: doesnt mat...\n",
            "Added akansha: a really c...\n",
            "Added Todd: a man that...\n",
            "Added Sexing: the act of...\n",
            "Added hit the bricks: 1.a someti...\n",
            "Added Mason: used to de...\n",
            "Added fb: fb is an a...\n",
            "Added soul patch: n. the bea...\n",
            "Added fanfiction.net: a stupid s...\n",
            "Added Bumvertising: when a mer...\n",
            "Added woo: a cigarett...\n",
            "Found a good pattern in the definition: slang for [baby mama], [wifey], [girlfriend]\n",
            "Added bussy baby: slang for ...\n",
            "Added Linsanity: the energy...\n",
            "Added Kate: girl that ...\n",
            "Added ash hole: when a bow...\n",
            "Added plush: hot, aweso...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# example: usage example,\n",
        "# upvotes: number of upvotes on Urban Dictionary,\n",
        "# downvotes: number of downvotes on Urban Dictionary\n",
        "import time\n",
        "wikitest = False # set to true to test the wikipedia page length\n",
        "# include a phrase if it has a combined total of at least 100 upvotes and downvotes on Urban Dictionary\n",
        "rand_dict = {}\n",
        "total_votes_thresh = 200 # min number of upvotes + downvotes\n",
        "upvotes_thresh = 100 # min number of upvotes\n",
        "downvotes_thresh = 10 # max number of downvotes\n",
        "desired_number_of_cards = 15\n",
        "rands = [] # the randoms\n",
        "\n",
        "while len(rand_dict) < desired_number_of_cards:\n",
        "  time.sleep(1)\n",
        "  rand = ud.random() # returns a list of 5 random phrases and definitions from Urban Dictionary\n",
        "  # append these to a master list\n",
        "  rands.extend(rand) # rands is a list of all the random phrases and definitions from Urban Dictionary\n",
        "\n",
        "  # iterate over the elements in the rand object\n",
        "  for element in rand:\n",
        "    # extract the relevant data from the element\n",
        "    phrase = element.word\n",
        "    definition = element.definition\n",
        "    usage_example = element.example\n",
        "    upvotes = element.upvotes\n",
        "    downvotes = element.downvotes\n",
        "    # define a list of boolean values\n",
        "    values = [upvotes + downvotes >= total_votes_thresh,\n",
        "              upvotes >= upvotes_thresh and downvotes <= downvotes_thresh,\n",
        "              check_for_good_patterns(definition, phrase)]\n",
        "    \n",
        "\n",
        "    # check if any element in the list is True\n",
        "    if any(values) and not check_for_badwords(definition, bad_patterns) \\\n",
        "      and not check_for_badwords(phrase, bad_patterns): # check if the phrase or definition contains any buzzwords\n",
        "    # include the element in the dictionary if it has a combined total of at least 100 upvotes and downvotes on Urban Dictionary\n",
        "      definition = remove_undesireable_sentences(\n",
        "        definition, phrase, bad_patterns) # remove sentences that contain buzzwords, are not in English, etc.\n",
        "      definition = unpack_definitions(definition) # remove brackets and clean up the definitions with regex\n",
        "      if wikitest:\n",
        "        print('Testing phrase:', phrase, ' for popularity on Wikipedia...')\n",
        "        page_length = get_page_length(phrase) # get the length of the Wikipedia page for the phrase\n",
        "        print('Page length:', page_length)\n",
        "        if page_length < 1000: # if the page is too short, skip it\n",
        "          continue\n",
        "      \n",
        "      print(f'Added {phrase}: {definition[0:10]}...')\n",
        "      rand_dict[phrase] = definition\n",
        "\n",
        "# save the cards to a csv file\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_dict(rand_dict, orient='index')\n",
        "df.to_csv('cards.csv', header=False)\n",
        "\n",
        "# save the cards to a json file\n",
        "import json\n",
        "with open('cards.json', 'w') as f:\n",
        "    json.dump(rand_dict, f)\n",
        "  \n",
        "# Hippopotomonstrosesquippedaliophobia - fear of long words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# initialize the sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def has_sexual_comment(phrase, bad_patterns):\n",
        "    # analyze the sentiment of the phrase\n",
        "    scores = analyzer.polarity_scores(phrase)\n",
        "    # check if the compound score is greater than or equal to 0.5, which indicates a positive sentiment\n",
        "    if scores['compound'] <= 0.5:\n",
        "        print(f' * Found a negative sentiment in the phrase: {phrase}')\n",
        "        print(f'Score: {scores}')\n",
        "    \n",
        "        # if the phrase also contains a buzzword, return True\n",
        "        if any(re.match(pattern, phrase) for pattern in bad_patterns):\n",
        "            print(f' ** Also, Found a bad pattern in the phrase: {phrase}')\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Found a negative sentiment in the phrase: a single or collection of females that you shamefully solicit for sexual acts but what be disparaged if anyone knew that you were hooking up with them.\n",
            "Score: {'neg': 0.174, 'neu': 0.826, 'pos': 0.0, 'compound': -0.6187}\n",
            "Testing for sex in the phrase, a single or collection of females that you shamefully solicit for sexual acts but what be disparaged if anyone knew that you were hooking up with them.: False\n"
          ]
        }
      ],
      "source": [
        "test_phrase = \"a single or collection of females that you shamefully solicit for sexual acts but what be disparaged if anyone knew that you were hooking up with them.\"\n",
        "\n",
        "has_sex = has_sexual_comment(test_phrase, bad_patterns)\n",
        "\n",
        "print('Testing for sex in the phrase, {}: {}'.format(test_phrase,has_sex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Found a negative sentiment in the phrase: another phrased coined by david alan weinshel wheeler 02 bu 06. simplifies the stress of saying how and come separately. also eliminates the need to hit hte spacebar when typing.\n",
            "Score: {'neg': 0.088, 'neu': 0.912, 'pos': 0.0, 'compound': -0.4215}\n",
            "Testing for sex in the phrase, another phrased coined by david alan weinshel wheeler 02 bu 06. simplifies the stress of saying how and come separately. also eliminates the need to hit hte spacebar when typing.: False\n"
          ]
        }
      ],
      "source": [
        "test_phrase =  \"another phrased coined by david alan weinshel wheeler 02 bu 06. simplifies the stress of saying how and come separately. also eliminates the need to hit hte spacebar when typing.\"\n",
        "has_sex = has_sexual_comment(test_phrase, bad_patterns)\n",
        "\n",
        "print('Testing for sex in the phrase, {}: {}'.format(test_phrase,has_sex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Earthquake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mat Ishbia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Specials</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Weather Chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Boston Bruins</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Green Bay Packers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Jalen Hurts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Giants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>New York Giants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Trump</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Willie McGinest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Harvey Weinstein</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Drew Griffin CNN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Aaron Rodgers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Tennessee Titans</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Seattle weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Pope Francis resignation letter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Gardner Minshew</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Los Angeles Lakers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Whitney Houston</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  0\n",
              "0                        Earthquake\n",
              "1                        Mat Ishbia\n",
              "2                      The Specials\n",
              "3                   Weather Chicago\n",
              "4                     Boston Bruins\n",
              "5                 Green Bay Packers\n",
              "6                       Jalen Hurts\n",
              "7                            Giants\n",
              "8                   New York Giants\n",
              "9                             Trump\n",
              "10                  Willie McGinest\n",
              "11                 Harvey Weinstein\n",
              "12                 Drew Griffin CNN\n",
              "13                    Aaron Rodgers\n",
              "14                 Tennessee Titans\n",
              "15                  Seattle weather\n",
              "16  Pope Francis resignation letter\n",
              "17                  Gardner Minshew\n",
              "18               Los Angeles Lakers\n",
              "19                  Whitney Houston"
            ]
          },
          "execution_count": 230,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "real_time_trending_searches_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "io8WjqvRdjkk"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_77763/3033970032.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# generate a deck of cards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mcard_deck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_a_card_deck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;31m# filter the deck for inappropriate content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_77763/3033970032.py\u001b[0m in \u001b[0;36mcreate_a_card_deck\u001b[0;34m(num_cards)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin_count_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"slang\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mslang_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_slang_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m       \u001b[0mnew_card\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslang_phrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"slang\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_definition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslang_phrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmin_count_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"people\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0mperson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_wiki_person\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_77763/3033970032.py\u001b[0m in \u001b[0;36mget_definition\u001b[0;34m(card)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# this is a word so use PyDictionary library.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdefinition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this could have mult. meanings returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdefinition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefinition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the first meaning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_random_slang_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import random\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from PyDictionary import PyDictionary\n",
        "import wikipedia\n",
        "\n",
        "dictionary = PyDictionary() # instantiate \n",
        "\n",
        "def get_wiki_definition(card):\n",
        "  # Get the summary of the Wikipedia page\n",
        "  summary = wikipedia.summary(card)\n",
        "  return summary\n",
        "\n",
        "# # Example usage:\n",
        "# card = \"John Smith\"\n",
        "# definition = get_definition(card)\n",
        "# print(definition)  # prints the summary of the Wikipedia page \"John Smith\"\n",
        "\n",
        "def get_definition(card):\n",
        "  # Check the type of the card\n",
        "  if is_slang_phrase(card):\n",
        "    # Get the definition of the slang phrase from Urban Dictionary\n",
        "    url = \"https://api.urbandictionary.com/v0/define?term=\" + card\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    definition = data['list'][0]['definition']\n",
        "  elif is_person(card):\n",
        "    # Get the summary of the person from Wikipedia\n",
        "    definition = get_wiki_definition(card[0]) # for a card title get the definition (summary) of the wikipedia page.\n",
        "  else:\n",
        "    # this is a word so use PyDictionary library.\n",
        "    definition = dictionary.meaning(str(card[0])) # this could have mult. meanings returned. \n",
        "    definition = definition[0] # the first meaning. \n",
        "\n",
        "def get_random_slang_phrase():\n",
        "  # Make a request to the Urban Dictionary API to get a random slang phrase\n",
        "  url = \"https://api.urbandictionary.com/v0/random\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  slang_phrase = data['list'][0]['word']\n",
        "  return slang_phrase\n",
        "\n",
        "def get_random_wiki_person():\n",
        "  # Make a request to Wikipedia API to get a random person\n",
        "  url = \"https://en.wikipedia.org/w/api.php?action=query&format=json&list=random&rnnamespace=0&rnlimit=1\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  person = data['query']['random'][0]['title']\n",
        "  return person\n",
        "\n",
        "def get_random_wiki_word():\n",
        "  # Make a request to Wikipedia API to get a random word\n",
        "  url = \"https://en.wikipedia.org/w/api.php?action=query&format=json&list=random&rnnamespace=0&rnlimit=1\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  word = data['query']['random'][0]['title']\n",
        "  return word\n",
        "\n",
        "def create_a_card_deck(num_cards=30):\n",
        "  # create 30 random cards with equal distributions of slang, people, and words.\n",
        "  # avoid profanity, or lude sexual topics.\n",
        "  \n",
        "  # Create a list of all the cards\n",
        "  cards = []\n",
        "\n",
        "  # Initialize a dictionary to keep track of the count of each type of card\n",
        "  counts = {\n",
        "    \"slang\": 0,\n",
        "    \"people\": 0,\n",
        "    \"words\": 0\n",
        "  }\n",
        "\n",
        "  # While there are fewer than 30 cards in the deck, generate a new card\n",
        "  while len(cards) < num_cards:\n",
        "    # Find the type of card with the lowest count\n",
        "    min_count_type = min(counts, key=counts.get)\n",
        "\n",
        "    # Generate a new card of that type\n",
        "    if min_count_type == \"slang\":\n",
        "      slang_phrase = get_random_slang_phrase()\n",
        "      new_card = (slang_phrase, \"slang\", get_definition(slang_phrase))\n",
        "    elif min_count_type == \"people\":\n",
        "      person = get_random_wiki_person()\n",
        "      new_card = (person, \"people\", get_definition(person))\n",
        "    else:\n",
        "      word = get_random_wiki_word()\n",
        "      new_card = (word, \"word\", get_definition(word))\n",
        "\n",
        "    # Add the new card to the list and update the count\n",
        "    cards.append(new_card)\n",
        "    counts[min_count_type] += 1 \n",
        "  return cards\n",
        "\n",
        "def is_slang_phrase(card):\n",
        "  if card[1] == 'slang':\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def is_person(card):\n",
        "  if card[1] == 'person':\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def filter_cards(cards):\n",
        "  # create a function to filter out inappropriate content\n",
        "  filtered_cards = []\n",
        "  for card in cards:\n",
        "\n",
        "    # check if the card contains profanity or sexual content\n",
        "    print(\"checking card: {}\".format(card))\n",
        "    definition = card[2] # definition\n",
        "    card_type = card[1] # i.e. 'slang'\n",
        "    word = card[0] # word \n",
        "    try:\n",
        "      if \"profanity\" in definition or \"sex\" in definition or \"lewd\" in definition:\n",
        "        continue\n",
        "      elif check_for_badwords():\n",
        "        continue\n",
        "      else:\n",
        "        filtered_cards.append(card)\n",
        "    except Exception as e:\n",
        "      print(f'error with finding the meaning in the soup object. soup comes from the urban dictionary page.')\n",
        "      print(e)\n",
        "  return filtered_cards\n",
        "\n",
        "# generate a deck of cards\n",
        "card_deck = create_a_card_deck()\n",
        "\n",
        "# filter the deck for inappropriate content\n",
        "filtered_deck = filter_cards(card_deck)\n",
        "\n",
        "# print the cards\n",
        "print('Generated {} cards'.format(len(filtered_deck)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tif9XZI5oRTR"
      },
      "outputs": [],
      "source": [
        "# print the filtered cards\n",
        "for card in filtered_deck:\n",
        "  print(card)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TcE6dW8-u3Ni"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "card = \"dope\"\n",
        "definition = get_definition(card)\n",
        "card_type = \"phrase\"\n",
        "card_preview = \n",
        "\n",
        "# (slang_phrase, \"slang\", get_definition(slang_phrase))\n",
        "    \n",
        "\n",
        "\n",
        "print(definition)  # prints the definition of the slang phrase \"dope\"\n",
        "\n",
        "card = \"John Smith\"\n",
        "definition = get_definition(card)\n",
        "print(definition)  # prints the summary of the person \"John Smith\"\n",
        "\n",
        "card = \"tree\"\n",
        "definition = get_definition(card)\n",
        "print(definition)  # prints the definition of the word \"tree\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
