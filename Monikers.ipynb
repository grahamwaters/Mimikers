{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_wgZ8DsdcYz",
        "outputId": "51680fd1-495e-40fa-b5bc-38c2fdc650b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (2.28.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.9/site-packages (4.11.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
            "Requirement already satisfied: wikipedia in /opt/anaconda3/lib/python3.9/site-packages (1.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from wikipedia) (2.28.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.9/site-packages (from wikipedia) (4.11.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n",
            "Requirement already satisfied: PyDictionary in /opt/anaconda3/lib/python3.9/site-packages (2.0.1)\n",
            "Requirement already satisfied: bs4 in /opt/anaconda3/lib/python3.9/site-packages (from PyDictionary) (0.0.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from PyDictionary) (2.28.1)\n",
            "Requirement already satisfied: goslate in /opt/anaconda3/lib/python3.9/site-packages (from PyDictionary) (1.5.4)\n",
            "Requirement already satisfied: click in /Users/grahamwaters/.local/lib/python3.9/site-packages (from PyDictionary) (8.1.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.9/site-packages (from bs4->PyDictionary) (4.11.1)\n",
            "Requirement already satisfied: futures in /opt/anaconda3/lib/python3.9/site-packages (from goslate->PyDictionary) (3.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests->PyDictionary) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests->PyDictionary) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->PyDictionary) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->PyDictionary) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->bs4->PyDictionary) (2.3.2.post1)\n",
            "Collecting pytrends\n",
            "  Downloading pytrends-4.8.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0 in /opt/anaconda3/lib/python3.9/site-packages (from pytrends) (2.28.1)\n",
            "Requirement already satisfied: pandas>=0.25 in /opt/anaconda3/lib/python3.9/site-packages (from pytrends) (1.4.4)\n",
            "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.9/site-packages (from pytrends) (4.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.25->pytrends) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.25->pytrends) (1.21.5)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.0->pytrends) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.0->pytrends) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.0->pytrends) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.0->pytrends) (1.26.13)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=0.25->pytrends) (1.16.0)\n",
            "Building wheels for collected packages: pytrends\n",
            "  Building wheel for pytrends (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pytrends: filename=pytrends-4.8.0-py3-none-any.whl size=16109 sha256=5e6e0dc3b97d8e909ac47b59a976b295dd8ffb6d14a332d35efddd468c0d3bfc\n",
            "  Stored in directory: /Users/grahamwaters/Library/Caches/pip/wheels/39/b3/27/fe5d0e140183e8001d8e09a9c290c5313490e3899f09b06a7f\n",
            "Successfully built pytrends\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install wikipedia\n",
        "!pip install PyDictionary\n",
        "!pip install pytrends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 10 trending searches.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>exploreQuery</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wordle</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Election results</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Betty White</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Queen Elizabeth</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bob Saget</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Ukraine</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mega Millions</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Powerball numbers</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Anne Heche</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Jeffrey Dahmer</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               title exploreQuery\n",
              "0             Wordle             \n",
              "1   Election results             \n",
              "2        Betty White             \n",
              "3    Queen Elizabeth             \n",
              "4          Bob Saget             \n",
              "5            Ukraine             \n",
              "6      Mega Millions             \n",
              "7  Powerball numbers             \n",
              "8         Anne Heche             \n",
              "9     Jeffrey Dahmer             "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pytrends\n",
        "from pytrends.request import TrendReq\n",
        "import wikipedia\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import PyDictionary\n",
        "from PyDictionary import PyDictionary\n",
        "#import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "dir(pytrends)\n",
        "pytrends = TrendReq(hl='en-US', tz=360) # create a pytrends object\n",
        "#pytrends.trending_searches(pn='united_states') # trending searches in real time for United States\n",
        "# create a dictionary of the top 1000 trending searches for this year in the United States (as of 2022)\n",
        "trending_searches_dict = pytrends.top_charts(\n",
        "    date='2022',    # specifies the time period as the last year\n",
        "    geo='US',            # specifies the country as the United States\n",
        ")\n",
        "real_time_trending_searches_dict = pytrends.trending_searches(pn='united_states') # trending searches in real time for United States\n",
        "related_topics_dict = pytrends.related_topics() # related topics for the top 10 trending searches in real time for United States\n",
        "\n",
        "\n",
        "# show how many trending searches there are\n",
        "print(f'Retrieved {len(trending_searches_dict)} trending searches.')\n",
        "# show the top 10 trending searches in real time for United States\n",
        "trending_searches_dict.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "related_topics_dict "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Earthquake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Specials</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Weather Chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Boston Bruins</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Green Bay Packers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>New York Giants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Trump</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Drew Griffin CNN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Harvey Weinstein</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Jalen Hurts injury</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Aaron Rodgers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Tennessee Titans</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Los Angeles Lakers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Pope Francis resignation letter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Seattle weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Whitney Houston</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Liz Cheney</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Afghanistan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Utah Jazz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Emmanuel Macron</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  0\n",
              "0                        Earthquake\n",
              "1                      The Specials\n",
              "2                   Weather Chicago\n",
              "3                     Boston Bruins\n",
              "4                 Green Bay Packers\n",
              "5                   New York Giants\n",
              "6                             Trump\n",
              "7                  Drew Griffin CNN\n",
              "8                  Harvey Weinstein\n",
              "9                Jalen Hurts injury\n",
              "10                    Aaron Rodgers\n",
              "11                 Tennessee Titans\n",
              "12               Los Angeles Lakers\n",
              "13  Pope Francis resignation letter\n",
              "14                  Seattle weather\n",
              "15                  Whitney Houston\n",
              "16                       Liz Cheney\n",
              "17                      Afghanistan\n",
              "18                        Utah Jazz\n",
              "19                  Emmanuel Macron"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "real_time_trending_searches_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RH2Tv5W-x909"
      },
      "outputs": [],
      "source": [
        "def check_for_badwords():\n",
        "  if any(re.search(r'\\b' + word + r'\\b', definition) for word in ['sex', 'porn','fuck','shit','damn','ass','cock','whore','nigger','slut','blowjob','faggot''boob', 'breast', 'cunt', 'pussy', 'dick', 'naked', 'nud*', 'nipple']):\n",
        "    return True\n",
        "  else:\n",
        "    return False # no bad words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "io8WjqvRdjkk"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_77763/3033970032.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# generate a deck of cards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mcard_deck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_a_card_deck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;31m# filter the deck for inappropriate content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_77763/3033970032.py\u001b[0m in \u001b[0;36mcreate_a_card_deck\u001b[0;34m(num_cards)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin_count_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"slang\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mslang_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_slang_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m       \u001b[0mnew_card\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslang_phrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"slang\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_definition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslang_phrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmin_count_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"people\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0mperson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_wiki_person\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_77763/3033970032.py\u001b[0m in \u001b[0;36mget_definition\u001b[0;34m(card)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# this is a word so use PyDictionary library.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdefinition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this could have mult. meanings returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdefinition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefinition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the first meaning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_random_slang_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import random\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from PyDictionary import PyDictionary\n",
        "import wikipedia\n",
        "\n",
        "dictionary = PyDictionary() # instantiate \n",
        "\n",
        "def get_wiki_definition(card):\n",
        "  # Get the summary of the Wikipedia page\n",
        "  summary = wikipedia.summary(card)\n",
        "  return summary\n",
        "\n",
        "# # Example usage:\n",
        "# card = \"John Smith\"\n",
        "# definition = get_definition(card)\n",
        "# print(definition)  # prints the summary of the Wikipedia page \"John Smith\"\n",
        "\n",
        "def get_definition(card):\n",
        "  # Check the type of the card\n",
        "  if is_slang_phrase(card):\n",
        "    # Get the definition of the slang phrase from Urban Dictionary\n",
        "    url = \"https://api.urbandictionary.com/v0/define?term=\" + card\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    definition = data['list'][0]['definition']\n",
        "  elif is_person(card):\n",
        "    # Get the summary of the person from Wikipedia\n",
        "    definition = get_wiki_definition(card[0]) # for a card title get the definition (summary) of the wikipedia page.\n",
        "  else:\n",
        "    # this is a word so use PyDictionary library.\n",
        "    definition = dictionary.meaning(str(card[0])) # this could have mult. meanings returned. \n",
        "    definition = definition[0] # the first meaning. \n",
        "\n",
        "def get_random_slang_phrase():\n",
        "  # Make a request to the Urban Dictionary API to get a random slang phrase\n",
        "  url = \"https://api.urbandictionary.com/v0/random\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  slang_phrase = data['list'][0]['word']\n",
        "  return slang_phrase\n",
        "\n",
        "def get_random_wiki_person():\n",
        "  # Make a request to Wikipedia API to get a random person\n",
        "  url = \"https://en.wikipedia.org/w/api.php?action=query&format=json&list=random&rnnamespace=0&rnlimit=1\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  person = data['query']['random'][0]['title']\n",
        "  return person\n",
        "\n",
        "def get_random_wiki_word():\n",
        "  # Make a request to Wikipedia API to get a random word\n",
        "  url = \"https://en.wikipedia.org/w/api.php?action=query&format=json&list=random&rnnamespace=0&rnlimit=1\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  word = data['query']['random'][0]['title']\n",
        "  return word\n",
        "\n",
        "def create_a_card_deck(num_cards=30):\n",
        "  # create 30 random cards with equal distributions of slang, people, and words.\n",
        "  # avoid profanity, or lude sexual topics.\n",
        "  \n",
        "  # Create a list of all the cards\n",
        "  cards = []\n",
        "\n",
        "  # Initialize a dictionary to keep track of the count of each type of card\n",
        "  counts = {\n",
        "    \"slang\": 0,\n",
        "    \"people\": 0,\n",
        "    \"words\": 0\n",
        "  }\n",
        "\n",
        "  # While there are fewer than 30 cards in the deck, generate a new card\n",
        "  while len(cards) < num_cards:\n",
        "    # Find the type of card with the lowest count\n",
        "    min_count_type = min(counts, key=counts.get)\n",
        "\n",
        "    # Generate a new card of that type\n",
        "    if min_count_type == \"slang\":\n",
        "      slang_phrase = get_random_slang_phrase()\n",
        "      new_card = (slang_phrase, \"slang\", get_definition(slang_phrase))\n",
        "    elif min_count_type == \"people\":\n",
        "      person = get_random_wiki_person()\n",
        "      new_card = (person, \"people\", get_definition(person))\n",
        "    else:\n",
        "      word = get_random_wiki_word()\n",
        "      new_card = (word, \"word\", get_definition(word))\n",
        "\n",
        "    # Add the new card to the list and update the count\n",
        "    cards.append(new_card)\n",
        "    counts[min_count_type] += 1 \n",
        "  return cards\n",
        "\n",
        "def is_slang_phrase(card):\n",
        "  if card[1] == 'slang':\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def is_person(card):\n",
        "  if card[1] == 'person':\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def filter_cards(cards):\n",
        "  # create a function to filter out inappropriate content\n",
        "  filtered_cards = []\n",
        "  for card in cards:\n",
        "\n",
        "    # check if the card contains profanity or sexual content\n",
        "    print(\"checking card: {}\".format(card))\n",
        "    definition = card[2] # definition\n",
        "    card_type = card[1] # i.e. 'slang'\n",
        "    word = card[0] # word \n",
        "    try:\n",
        "      if \"profanity\" in definition or \"sex\" in definition or \"lewd\" in definition:\n",
        "        continue\n",
        "      elif check_for_badwords():\n",
        "        continue\n",
        "      else:\n",
        "        filtered_cards.append(card)\n",
        "    except Exception as e:\n",
        "      print(f'error with finding the meaning in the soup object. soup comes from the urban dictionary page.')\n",
        "      print(e)\n",
        "  return filtered_cards\n",
        "\n",
        "# generate a deck of cards\n",
        "card_deck = create_a_card_deck()\n",
        "\n",
        "# filter the deck for inappropriate content\n",
        "filtered_deck = filter_cards(card_deck)\n",
        "\n",
        "# print the cards\n",
        "print('Generated {} cards'.format(len(filtered_deck)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tif9XZI5oRTR"
      },
      "outputs": [],
      "source": [
        "# print the filtered cards\n",
        "for card in filtered_deck:\n",
        "  print(card)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TcE6dW8-u3Ni"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "card = \"dope\"\n",
        "definition = get_definition(card)\n",
        "card_type = \"phrase\"\n",
        "card_preview = \n",
        "\n",
        "# (slang_phrase, \"slang\", get_definition(slang_phrase))\n",
        "    \n",
        "\n",
        "\n",
        "print(definition)  # prints the definition of the slang phrase \"dope\"\n",
        "\n",
        "card = \"John Smith\"\n",
        "definition = get_definition(card)\n",
        "print(definition)  # prints the summary of the person \"John Smith\"\n",
        "\n",
        "card = \"tree\"\n",
        "definition = get_definition(card)\n",
        "print(definition)  # prints the definition of the word \"tree\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
