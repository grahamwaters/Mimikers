{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_wgZ8DsdcYz",
        "outputId": "51680fd1-495e-40fa-b5bc-38c2fdc650b5"
      },
      "outputs": [],
      "source": [
        "# !pip install requests\n",
        "# !pip install beautifulsoup4\n",
        "# !pip install wikipedia\n",
        "# !pip install PyDictionary\n",
        "# !pip install pytrends\n",
        "# !pip install urbandictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 10 trending searches.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>exploreQuery</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wordle</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Election results</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Betty White</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Queen Elizabeth</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bob Saget</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Ukraine</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mega Millions</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Powerball numbers</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Anne Heche</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Jeffrey Dahmer</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               title exploreQuery\n",
              "0             Wordle             \n",
              "1   Election results             \n",
              "2        Betty White             \n",
              "3    Queen Elizabeth             \n",
              "4          Bob Saget             \n",
              "5            Ukraine             \n",
              "6      Mega Millions             \n",
              "7  Powerball numbers             \n",
              "8         Anne Heche             \n",
              "9     Jeffrey Dahmer             "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pytrends\n",
        "from pytrends.request import TrendReq\n",
        "import wikipedia\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import PyDictionary\n",
        "from PyDictionary import PyDictionary\n",
        "#import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "dir(pytrends)\n",
        "pytrends = TrendReq(hl='en-US', tz=360) # create a pytrends object\n",
        "#pytrends.trending_searches(pn='united_states') # trending searches in real time for United States\n",
        "# create a dictionary of the top 1000 trending searches for this year in the United States (as of 2022)\n",
        "trending_searches_dict = pytrends.top_charts(\n",
        "    date='2022',    # specifies the time period as the last year\n",
        "    geo='US',            # specifies the country as the United States\n",
        ")\n",
        "real_time_trending_searches_dict = pytrends.trending_searches(pn='united_states') # trending searches in real time for United States\n",
        "\n",
        "\n",
        "\n",
        "# show how many trending searches there are\n",
        "print(f'Retrieved {len(trending_searches_dict)} trending searches.')\n",
        "# show the top 10 trending searches in real time for United States\n",
        "trending_searches_dict.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urbandictionary as ud\n",
        "import re\n",
        "# adding import alias recommended\n",
        "bad_patterns = [r'sex*', r'porn*',r'fuck*',r'-ass*','ass','shit',r'damn*',r'ass|asse*',r'cock*',r'whor*',r'nigg*',r'slut*','blowjob',r'fagg*',r'boob|boob*', r'bitch*', r'bastard*',' ho |hoe',r'breast*|jugs', r'cunt*', r'puss*', r'dick*', 'naked', r'nud*', r'masterb*',r'mastu',r'nipple*',r'penis|penal|peni*','god','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','ejaculate','jew*','islamic','atheist',r'rapist*|rape*',r'pedo*','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','gay',r'tit*|titt*','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','islamic','atheist','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','yahwey','yeshua',r'israel*',r'sex*', r'porn*',r'fuck*',r'-ass*','ass','shit',r'damn*',r'ass|asse*',r'cock*',r'whor*',r'nigg*',r'slut*','blowjob','bj',r'fagg*',r'boob|boob*', r'breast*|jugs', r'cunt*', r'puss*', r'dick*', 'naked', r'nud*', r'nipple*',r'penis|penal|peni*','jesus','christ','bible','church','religion','pray','prayer','faith','lord','gay',r'tit*|titt*', 'fellatio', 'fuck', 'nigger','lynch',r'erotic*','screw','lay','right wing','black man','black girl','nigga','racial']\n",
        "\n",
        "# to make the game more fun we can replace words with less inflamatory ones.\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "\n",
        "def check_for_badwords(definition, bad_patterns):\n",
        "    # if any of the buzzwords are found return a list of the matches, else an empty list\n",
        "    # include porter stemmer to match variations of the words in the bad_patterns list\n",
        "    from nltk.stem import PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in bad_patterns if len(stemmer.stem(word)) >= 4]\n",
        "    # only include the stemmed words which begin with the same letter as the original word (this is to avoid adding words that are not related to the original word)\n",
        "    stemmed_words = [word for word in stemmed_words if word[0] == word[0]]\n",
        "    bad_patterns.extend(stemmed_words)\n",
        "    # only include stemmed words that are not already in the list of words\n",
        "    bad_patterns = list(set(bad_patterns))\n",
        "    # create a list of regex patterns that match the words in the list\n",
        "    patterns = [re.compile(rf'\\b{word}\\b', re.IGNORECASE) for word in bad_patterns]\n",
        "    # return the list of regex patterns (regex patterns are used to match words, and are more efficient than using the 'in' operator)\n",
        "    \n",
        "    matches = [match.group(0) for match in re.finditer(r'\\b(?:' + '|'.join(bad_patterns) + r')\\b', definition, re.IGNORECASE)]\n",
        "    if len(matches)>0:\n",
        "        print(f'Found bad words in definition: {matches}')\n",
        "    return matches\n",
        "\n",
        "\n",
        "def check_for_good_patterns(definition, title):\n",
        "    # check both title and definition for good patterns\n",
        "    good_patterns = [r'\\b\\w+phobia\\.?\\b', r'\\bslang\\b', r'\\bacronymn\\b', r'\\bmeme\\b']\n",
        "\n",
        "\n",
        "    if any(re.match(r'\\b' + word + r'\\b', definition) for word in good_patterns):\n",
        "        print(f'\\nFound a good pattern in the definition: {definition}')\n",
        "        return True\n",
        "    elif any(re.match(r'\\b' + word + r'\\b', title) for word in good_patterns):\n",
        "        print(f'Found a good pattern in the title: {title}')\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def remove_undesireable_sentences(definition, title, bad_patterns):\n",
        "    # convert the definition and title to lowercase\n",
        "    definition = definition.lower()\n",
        "    title = title.lower()\n",
        "\n",
        "    # split the definition into sentences\n",
        "    sentences = definition.split('.')\n",
        "\n",
        "    # split the title into words\n",
        "    title_words = title.split()\n",
        "\n",
        "    # initialize a list to store the acceptable sentences\n",
        "    acceptable_sentences = []\n",
        "\n",
        "    # iterate over the sentences in the definition\n",
        "    for sentence in sentences:\n",
        "        # split the sentence into words\n",
        "        sentence_words = sentence.split()\n",
        "\n",
        "        # check if any of the words in the sentence are also in the title\n",
        "        if not any(word in title_words for word in sentence_words):\n",
        "            # if not, add the sentence to the list of acceptable sentences\n",
        "            acceptable_sentences.append(sentence)\n",
        "\n",
        "    # remove sentences that are not in English\n",
        "    definition = re.sub(r'[^\\x00-\\x7f]',r'', '.'.join(acceptable_sentences))\n",
        "    # remove sentences that contain a regex match to any word in the buzzwords list\n",
        "    definition = re.sub(r'|'.join(map(re.escape, bad_patterns)), '', definition)\n",
        "    return definition\n",
        "\n",
        "replacements = {\n",
        "    r'\\bsex\\b': 'affection',\n",
        "    r'\\bporn\\w*\\b': 'adult entertainment',\n",
        "    r'fuck': 'have intimate relations with',\n",
        "    r'fucking': 'romancing, physically',\n",
        "    r'\\bfuck\\w*\\b': ' (bleep) ',\n",
        "    r'\\bass\\b': 'dump truck',\n",
        "    r'\\bshit\\w*\\b': 'poop',\n",
        "    r'damn\\w*\\b': 'darn',\n",
        "    r'god-damn\\w*\\b|goddamn\\w*\\b': 'super darn',\n",
        "    r'ass|asse*': 'rear end',\n",
        "    r'cock': 'rooster',\n",
        "    r'small dick energy':'little man syndrome',\n",
        "    r'\\bdick\\w*\\b': ' johnson ',\n",
        "    r'ppl':'people',\n",
        "    'wat':'what',\n",
        "    r'a black': 'an african-american',\n",
        "    r'black person':'dark skinned person',\n",
        "    r'pee' : 'urinate',\n",
        "    r'piss': 'urinate',\n",
        "    r'\\b(peeing|pissing)': 'urinating',\n",
        "    r'white person':'caucasian',\n",
        "    r'hole': ' cave ',\n",
        "    r'\\bwhore\\w*\\b': 'prostitute',\n",
        "    r'nigg*': 'racial slur',\n",
        "    '\\r\\n':' ',\n",
        "    r'slut': ' loose person ',\n",
        "    r'blowjob': 'random gift',\n",
        "    r'racist': 'ignorant',\n",
        "    'racism':'fear of people that look different',\n",
        "    r'faggot': 'gay person',\n",
        "    r'\\bfag\\w*\\b':'gay person',\n",
        "    r'boob|boob*|breast': 'lady pillows',\n",
        "    r'\\bbitch*\\b': 'mean woman',\n",
        "    r'bastard*': 'illegitimate child',\n",
        "    r'hoes?': 'chicks',\n",
        "    r'breast*|jugs': 'chest',\n",
        "    r'\\bcunt*\\b': 'comtemptible person',\n",
        "    r'\\bpuss\\w*\\b': 'vagina',\n",
        "    r'\\wdick*': 'penis',\n",
        "    r'naked': 'disrobed',\n",
        "    r'\\bnud\\w*\\b': 'unclothed',\n",
        "    r'\\nmasterbate\\w*\\b': 'self-gratified',\n",
        "    r'\\bmasturbating\\w*\\b': 'gratifying themselves',\n",
        "    r'\\b\\w{4}ilf\\b': 'person id like to get to know',\n",
        "    r'mastu\\w*\\b': 'self-gratification',\n",
        "    'god':'deity',\n",
        "    'jesus':'religious figure',\n",
        "    'christ':'religious figure',\n",
        "    'bible':'religious text',\n",
        "    'church':'place of worship',\n",
        "    'religion':'set of beliefs',\n",
        "    r'\\bpray\\w*\\b':'communicate with a deity',\n",
        "    'prayer':'a conversation with a deity',\n",
        "    'faith':'belief',\n",
        "    ' lord ':'captain',\n",
        "    ' allah ':'a diety',\n",
        "    'gay':'homosexual',\n",
        "    r' rapist ': ' intense toucher ',\n",
        "    r' rape ': ' unwelcomed tickling ', # eek I know... \n",
        "    r'pedo\\w*\\b': 'seventies mustached van driver',\n",
        "    'yahwey':'a deity',\n",
        "    'yeshua':'a religious figure',\n",
        "    ' sexual':' reproductive',\n",
        "    'douche':'chad',\n",
        "    ' sex ':' private adult time ',\n",
        "    'milf':\"mother I would like to take on a date\",\n",
        "    ' butt ':\"dumptruck/tailgate\"\n",
        "}\n",
        "\n",
        "def replacer_censor(definition,phrase,replacements_dict):\n",
        "    # Iterate over the keys in the replacements dictionary\n",
        "    for pattern in replacements_dict:\n",
        "        # Use re.sub to replace the occurrences of the pattern with its corresponding value in the phrase and definition strings\n",
        "        phrase = re.sub(pattern, replacements_dict[pattern], phrase)\n",
        "        definition = re.sub(pattern, replacements_dict[pattern], definition)\n",
        "    return phrase, definition\n",
        "\n",
        "def unpack_definitions(phrase,definition):\n",
        "    # remove the brackets and clean up the definitions\n",
        "    # with regex\n",
        "    definition = definition.replace(\"[\",\"\")\n",
        "    definition = definition.replace(\"]\",\"\")\n",
        "    definition = definition.replace(\"'\",\"\")\n",
        "    definition = definition.replace('\"',\"\")\n",
        "    definition = definition.replace(\"(\",\"\")\n",
        "    definition = definition.replace(\")\",\"\")\n",
        "\n",
        "    # remove double spaces\n",
        "    definition = definition.replace('  ',' ')\n",
        "    # also remove any .. by replacing them with a single .\n",
        "    definition = definition.replace('..','.')\n",
        "    # remove all non-ascii characters\n",
        "    definition = re.sub(r'[^\\x00-\\x7f]',r'', definition)\n",
        "    # remove any words that are not in the english dictionary\n",
        "    #english_words = set(w.lower() for w in nltk.corpus.words.words())\n",
        "    #definition = re.sub(r'\\b\\w+\\b', lambda m: m.group(0) if m.group(0) in english_words else '', definition)\n",
        "    # remove extra spaces\n",
        "    #definition = re.sub(' +', ' ', definition)\n",
        "    \n",
        "    phrase, definition = replacer_censor(definition,phrase,replacements)\n",
        "    \n",
        "\n",
        "    return phrase, definition\n",
        "\n",
        "# import wikipedia\n",
        "\n",
        "# def get_page_length(phrase):\n",
        "#   # search for pages on Wikipedia that match the given phrase\n",
        "#     try:\n",
        "#         pages = wikipedia.search(phrase)\n",
        "\n",
        "#         # retrieve the first page from the search results\n",
        "#         page = wikipedia.page(pages[0])\n",
        "#         print(f'Found the page for {page.title}', end='')\n",
        "#         if phrase.lower() != page.title.lower():\n",
        "#             print(' ... nevermind... not the right page.')\n",
        "#             return 0\n",
        "#         # return the length of the page\n",
        "#         return len(page.content)\n",
        "#     except Exception as e:\n",
        "#             return 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def prepper(words):\n",
        "#     # create a list of regex patterns that match the words in the list\n",
        "#     patterns = [re.compile(rf'\\b{word}\\b', re.IGNORECASE) for word in words]\n",
        "\n",
        "#     # add any variations of the words to the list of words by porter stemming\n",
        "#     from nltk.stem import PorterStemmer\n",
        "#     stemmer = PorterStemmer()\n",
        "#     stemmed_words = [stemmer.stem(word) for word in words if len(stemmer.stem(word)) >= 4]\n",
        "#     # only include the stemmed words which begin with the same letter as the original word (this is to avoid adding words that are not related to the original word)\n",
        "#     stemmed_words = [word for word in stemmed_words if word[0] == word[0]]\n",
        "#     words.extend(stemmed_words)\n",
        "#     # only include stemmed words that are not already in the list of words\n",
        "#     words = list(set(words))\n",
        "#     # create a list of regex patterns that match the words in the list\n",
        "#     patterns = [re.compile(rf'\\b{word}\\b', re.IGNORECASE) for word in words]\n",
        "#     # return the list of regex patterns (regex patterns are used to match words, and are more efficient than using the 'in' operator)\n",
        "#     return patterns\n",
        "\n",
        "\n",
        "\n",
        "# # create a function that takes a list of words and a list of regex patterns and returns a list of words that match the regex patterns\n",
        "# def match_words(words):\n",
        "#     words = words.split() # split the words string into a list of words\n",
        "#     try:\n",
        "#         patterns = prepper(words) # create the regex patterns\n",
        "#         matched_words = [] # initialize a list to store the matched words (words that match the regex patterns and indicate a bad word)\n",
        "#         for pattern in patterns: # iterate over the patterns (regex patterns)\n",
        "#             for word in words: # iterate over the words in the list of words.\n",
        "#                 if pattern.match(word): # if the word matches the regex pattern, add it to the list of matched words, and print a message to the console.\n",
        "#                     matched_words.append(word) # add the word to the list of matched words, if it matches the regex pattern\n",
        "#         return matched_words, patterns\n",
        "#     except Exception as e:\n",
        "#         print(e)\n",
        "#         return [],[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "blacklist = ['fuck','cunt','rape','kill']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "view: up(58) down(35) +++  Added to dictionary\n",
            "SWAX: up(4) down(8) +++  Added to dictionary\n",
            "penis pilgrim: up(5) down(1) +++  Added to dictionary\n",
            "Gelfmania: up(5) down(0) +++  Added to dictionary\n",
            "CMB: up(20) down(47) +++  Added to dictionary\n",
            "Glumagen: up(2) down(0) +++  Added to dictionary\n",
            "Armchair english teacher: up(1) down(1) +++  Added to dictionary\n",
            "Loserville: up(58) down(20) +++  Added to dictionary\n",
            "Cuban Sunrise: up(20) down(13) +++  Added to dictionary\n",
            "Young bahhs: up(2) down(1) +++  Added to dictionary\n",
            "2tooty2: up(2) down(1) +++  Added to dictionary\n",
            "BÃ©la: up(114) down(44) +++  Added to dictionary\n",
            "HYBB: up(7) down(15) +++  Added to dictionary\n",
            "hot 20: up(7) down(0) +++  Added to dictionary\n",
            "Pluralis: up(1) down(1) +++  Added to dictionary\n",
            "urp: up(9) down(31) +++  Added to dictionary\n",
            "forbidden definitions: up(3) down(3) +++  Added to dictionary\n",
            "*#$!@#\n",
            "Rick'd up: up(1) down(1) +++  Added to dictionary\n",
            "Handle: up(60) down(57) +++  Added to dictionary\n",
            "Shot up doubles: up(1) down(0) +++  Added to dictionary\n",
            "Connor: up(7) down(36) +++  Added to dictionary\n",
            "Millions Knives: up(8) down(9) +++  Added to dictionary\n",
            "roflstick: up(2) down(3) +++  Added to dictionary\n",
            "Slow Turtle: up(15) down(0) +++  Added to dictionary\n",
            "Browzy: up(6) down(1) +++  Added to dictionary\n",
            "Toss the tit: up(1) down(0) +++  Added to dictionary\n",
            "Flap Jacks: up(10) down(7) +++  Added to dictionary\n",
            "man stealer: up(41) down(4) +++  Added to dictionary\n",
            "Twin Bush: up(1) down(2) +++  Added to dictionary\n",
            "Poo Pillow: up(16) down(1) +++  Added to dictionary\n",
            "asshole academy: up(24) down(2) +++  Added to dictionary\n",
            "Snatch smasher: up(1) down(0) +++  Added to dictionary\n",
            "jenking: up(37) down(29) +++  Added to dictionary\n",
            "chessed: up(37) down(8) +++  Added to dictionary\n",
            "Patrycja: up(5) down(37) +++  Added to dictionary\n",
            "National Poop Pic Day: up(4) down(0) +++  Added to dictionary\n",
            "jeff bettencourt: up(3) down(2) +++  Added to dictionary\n",
            "gubbala: up(1) down(0) +++  Added to dictionary\n",
            "tornadian: up(18) down(0) +++  Added to dictionary\n",
            "Rudabega Bean Dip: up(4) down(0) +++  Added to dictionary\n",
            "Unwomanplayafiedism: up(1) down(7) +++  Added to dictionary\n",
            "Grisdale: up(3) down(0) +++  Added to dictionary\n",
            "Siyah: up(6) down(2) +++  Added to dictionary\n",
            "Clit Tac: up(5) down(0) +++  Added to dictionary\n",
            "Tongsta: up(7) down(4) +++  Added to dictionary\n",
            "naggin-bottle: up(18) down(7) +++  Added to dictionary\n",
            "TEU POWER PLAY: up(16) down(1) +++  Added to dictionary\n",
            "Cat: up(2) down(1) +++  Added to dictionary\n",
            "Tj: up(4) down(6) +++  Added to dictionary\n",
            "lashayuh: up(1) down(0) +++  Added to dictionary\n",
            "Splick Spling: up(3) down(0) +++  Added to dictionary\n",
            "Heebing: up(2) down(0) +++  Added to dictionary\n",
            "Number of phrases in the dictionary: 52\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# example: usage example,\n",
        "# upvotes: number of upvotes on Urban Dictionary,\n",
        "# downvotes: number of downvotes on Urban Dictionary\n",
        "import time\n",
        "import random\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from tqdm import tqdm\n",
        "# import RateLimitException from UrbanDictionary\n",
        "\n",
        "wikitest = False # set to true to test the wikipedia page length\n",
        "# include a phrase if it has a combined total of at least 100 upvotes and downvotes on Urban Dictionary\n",
        "rand_dict = {}\n",
        "total_votes_thresh = 200 # min number of upvotes + downvotes\n",
        "upvotes_thresh = 100 # min number of upvotes\n",
        "downvotes_thresh = total_votes_thresh//4 # maximum number of downvotes\n",
        "desired_number_of_cards = 50\n",
        "rands = [] # the randoms\n",
        "\n",
        "@limits(calls=1, period=10) # limit the number of calls to 1 per ten seconds\n",
        "def rate_limited_retrieval():\n",
        "  # create a list of words that are not allowed in the definitions of the cards\n",
        "  rand = ud.random() # returns a list of 5 random phrases and definitions from Urban Dictionary\n",
        "  return rand\n",
        "\n",
        "while len(rand_dict) < desired_number_of_cards:\n",
        "  # get a list of 5 random phrases and definitions from Urban Dictionary\n",
        "  try:\n",
        "    rand = rate_limited_retrieval() # returns a list of 5 random phrases and definitions from Urban Dictionary\n",
        "    # print('success')\n",
        "  except Exception as e:\n",
        "    #*print('Rate limit exceeded or this error: {}. \\nWaiting 10 seconds...'.format(e))\n",
        "    time.sleep(random.randint(5,10))\n",
        "    continue\n",
        "  #*print(f'Got {len(rand)} random phrases from Urban Dictionary')\n",
        "  # append these to a master list\n",
        "  rands.extend(rand) # rands is a list of all the random phrases and definitions from Urban Dictionary\n",
        "\n",
        "  # iterate over the elements in the rand object\n",
        "  for element in rand:\n",
        "    # extract the relevant data from the element\n",
        "    phrase = element.word\n",
        "    # check for any blacklist word in phrase.lower() and if so, skip\n",
        "    if any(word in phrase.lower() for word in blacklist):\n",
        "      print(phrase, ' *#$!@#')\n",
        "      continue\n",
        "    definition = element.definition\n",
        "    usage_example = element.example\n",
        "    upvotes = element.upvotes\n",
        "    downvotes = element.downvotes\n",
        "    print(f'{phrase}: up({upvotes}) down({downvotes})', end='')\n",
        "    # define a list of boolean values\n",
        "    values = [upvotes + downvotes >= total_votes_thresh,\n",
        "              upvotes >= upvotes_thresh and downvotes <= downvotes_thresh,\n",
        "              check_for_good_patterns(definition, phrase),\n",
        "              (upvotes > downvotes and phrase.find(' ') > 0)]\n",
        "    #!print(phrase, ':', definition[0:30], '...')\n",
        "\n",
        "    # Go ahead and save the phrase and definition if the phrase and definition are not already in the dictionary, we will filter later.\n",
        "    if phrase in rand_dict.keys(): # check if the phrase is already in the dictionary\n",
        "      print('-')\n",
        "      continue\n",
        "    else: # if the phrase is not in the dictionary, add it, and unpack the definition\n",
        "      rand_dict[phrase] = unpack_definitions(phrase,definition)[1] # remove brackets and clean up the definitions with regex\n",
        "      print(' +++  Added to dictionary')\n",
        "\n",
        "    #*print(f'{phrase}: {definition}...')\n",
        "    #^ Phase 2 (filtering)\n",
        "    # check if any element in the list is True\n",
        "    if any(values) and not check_for_badwords(definition, bad_patterns) and not check_for_badwords(phrase, bad_patterns): # check if the phrase or definition contains any buzzwords\n",
        "      # include the element in the dictionary if it has a combined total of at least 100 upvotes and downvotes on Urban Dictionary\n",
        "      definition = remove_undesireable_sentences(definition, phrase, bad_patterns) # remove sentences that contain buzzwords, are not in English, etc.\n",
        "      definition = unpack_definitions(phrase,definition) # remove brackets and clean up the definitions with regex\n",
        "\n",
        "      print(f'-- Added {phrase}: {definition}...')\n",
        "      rand_dict[phrase] = definition\n",
        "\n",
        "\n",
        "# print the number of phrases in the dictionary\n",
        "print(f'Number of phrases in the dictionary: {len(rand_dict)}')\n",
        "import json\n",
        "# save the dictionary to a json file\n",
        "with open('cards.json', 'w') as f:\n",
        "  json.dump(rand_dict, f)\n",
        "\n",
        "\n",
        "\n",
        "#! Phase 3 (filtering)\n",
        "\n",
        "# open the latest cards.json file and load it into a dictionary\n",
        "import json\n",
        "with open('cards.json', 'r') as f:\n",
        "    cards = json.load(f)\n",
        "\n",
        "# convert to pandas dataframe\n",
        "df = pd.DataFrame.from_dict(cards, orient='index')\n",
        "# convert the index to a column\n",
        "df.reset_index(inplace=True)\n",
        "# rename the columns\n",
        "df.columns = ['phrase', 'definition']\n",
        "\n",
        "# now go through every phrase, and definition and replace instances of any keys that exist in the replacements dict with the corresponding value.\n",
        "for i in range(len(df)):\n",
        "    phrase = df.loc[i, 'phrase']\n",
        "    definition = df.loc[i, 'definition']\n",
        "    phrase, definition = replacer_censor(definition, phrase, replacements)\n",
        "    df.loc[i, 'phrase'] = phrase\n",
        "    df.loc[i, 'definition'] = definition\n",
        "\n",
        "# convert the dataframe back to a dictionary\n",
        "cards = df.set_index('phrase').T.to_dict('list')\n",
        "\n",
        "# save the dictionary to a json file\n",
        "with open('cards.json', 'w') as f:\n",
        "    json.dump(cards, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Number of phrases in the dictionary: {len(rand_dict)}')\n",
        "import json\n",
        "# save the dictionary to a json file\n",
        "with open('cards.json', 'w') as f:\n",
        "  json.dump(rand_dict, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # open the latest cards.json file and load it into a dictionary\n",
        "# import json\n",
        "# with open('cards.json', 'r') as f:\n",
        "#     cards = json.load(f)\n",
        "\n",
        "# # convert to pandas dataframe\n",
        "# df = pd.DataFrame.from_dict(cards, orient='index')\n",
        "# # convert the index to a column\n",
        "# df.reset_index(inplace=True)\n",
        "# # rename the columns\n",
        "# df.columns = ['phrase', 'definition']\n",
        "\n",
        "# # now go through every phrase, and definition and replace instances of any keys that exist in the replacements dict with the corresponding value.\n",
        "# for i in range(len(df)):\n",
        "#     phrase = df.loc[i, 'phrase']\n",
        "#     definition = df.loc[i, 'definition']\n",
        "#     phrase, definition = replacer_censor(definition, phrase, replacements)\n",
        "#     df.loc[i, 'phrase'] = phrase\n",
        "#     df.loc[i, 'definition'] = definition\n",
        "\n",
        "# # convert the dataframe back to a dictionary\n",
        "# cards = df.set_index('phrase').T.to_dict('list')\n",
        "\n",
        "# # save the dictionary to a json file\n",
        "# with open('cards.json', 'w') as f:\n",
        "#     json.dump(cards, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# save the cards to a csv file\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_dict(rand_dict, orient='index')\n",
        "df.to_csv('cards.csv', header=False)\n",
        "\n",
        "# save the cards to a json file\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "try:\n",
        "  with open('cards_{}.json'.format(str(datetime.datetime.now())), 'w') as f:\n",
        "      json.dump(rand_dict, f)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "  with open('saved_cards.json', 'w') as f:\n",
        "      json.dump(rand_dict, f)\n",
        "\n",
        "  \n",
        "# Hippopotomonstrosesquippedaliophobia - fear of long words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#todo --- test this before using it\n",
        "def check_description_for_leaks(description, phrase):\n",
        "    # check if the description contains any of the words in the phrase, if so change the words to similar words, and return the new description using the same capitalization as the original description, and nltk to tokenize the words.\n",
        "    # tokenize the description and phrase\n",
        "    description_words = word_tokenize(description)\n",
        "    phrase_words = word_tokenize(phrase)\n",
        "\n",
        "    # loop through the phrase words and check if they are in the description\n",
        "    for i, word in enumerate(description_words):\n",
        "        if word in phrase_words:\n",
        "            # replace the word with a similar word\n",
        "            description_words[i] = 'XXXX'\n",
        "\n",
        "    # join the modified words back into a single string\n",
        "    modified_description = ' '.join(description_words)\n",
        "\n",
        "    # loop through the phrase words and modify the capitalization in the modified description\n",
        "    for word in phrase_words:\n",
        "        # find the index of the original word in the description\n",
        "        index = description.find(word)\n",
        "        # get the capitalization of the original word\n",
        "        capitalization = description[index:index+len(word)]\n",
        "        # modify the capitalization of the XXXX string\n",
        "        if capitalization.isupper():\n",
        "            modified_word = 'XXXX'.upper()\n",
        "        elif capitalization.istitle():\n",
        "            modified_word = 'XXXX'.title()\n",
        "        else:\n",
        "            modified_word = 'XXXX'\n",
        "        # replace the original word with the modified XXXX string\n",
        "        modified_description = modified_description.replace(word, modified_word, 1)\n",
        "\n",
        "    return modified_description\n",
        "\n",
        "\n",
        "def check_for_badwords(definition, bad_patterns):\n",
        "    # if any of the buzzwords are found return a list of the matches, else an empty list\n",
        "    # include porter stemmer to match variations of the words in the bad_patterns list\n",
        "    from nltk.stem import PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in bad_patterns if len(stemmer.stem(word)) >= 4]\n",
        "    # only include the stemmed words which begin with the same letter as the original word (this is to avoid adding words that are not related to the original word)\n",
        "    stemmed_words = [word for word in stemmed_words if word[0] == word[0]]\n",
        "    bad_patterns.extend(stemmed_words)\n",
        "    # only include stemmed words that are not already in the list of words\n",
        "    bad_patterns = list(set(bad_patterns))\n",
        "    # create a list of regex patterns that match the words in the list\n",
        "    patterns = [re.compile(rf'\\b{word}\\b', re.IGNORECASE) for word in bad_patterns]\n",
        "    # return the list of regex patterns (regex patterns are used to match words, and are more efficient than using the 'in' operator)\n",
        "    \n",
        "    matches = [match.group(0) for match in re.finditer(r'\\b(?:' + '|'.join(bad_patterns) + r')\\b', definition, re.IGNORECASE)]\n",
        "    print('Buzzwords found:', matches)\n",
        "    return matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing the patterns\n",
        "test = \"a racial slur used by nigga ignorant hypocritical right wing retards. if you think about it, we are all illegals, the ancestors came sex into north america the same way that mexicans are entering into our country and becoming citizens now. so if you have a problem with this, then you are a racist hypocrite!\"\n",
        "\n",
        "check_for_badwords(test, bad_patterns=bad_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# initialize the sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def has_sexual_comment(phrase, bad_patterns):\n",
        "    # analyze the sentiment of the phrase\n",
        "    scores = analyzer.polarity_scores(phrase)\n",
        "    # check if the compound score is greater than or equal to 0.5, which indicates a positive sentiment\n",
        "    if scores['compound'] <= 0.5:\n",
        "        print(f' * Found a negative sentiment in the phrase: {phrase}')\n",
        "        print(f'Score: {scores}')\n",
        "    \n",
        "        # if the phrase also contains a buzzword, return True\n",
        "        if any(re.match(pattern, phrase) for pattern in bad_patterns):\n",
        "            print(f' ** Also, Found a bad pattern in the phrase: {phrase}')\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_phrase = \"a single or collection of females that you shamefully solicit for sexual acts but what be disparaged if anyone knew that you were hooking up with them.\"\n",
        "\n",
        "has_sex = has_sexual_comment(test_phrase, bad_patterns)\n",
        "\n",
        "print('Testing for sex in the phrase, {}: {}'.format(test_phrase,has_sex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_phrase =  \"another phrased coined by david alan weinshel wheeler 02 bu 06. simplifies the stress of saying how and come separately. also eliminates the need to hit hte spacebar when typing.\"\n",
        "has_sex = has_sexual_comment(test_phrase, bad_patterns)\n",
        "\n",
        "print('Testing for sex in the phrase, {}: {}'.format(test_phrase,has_sex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_time_trending_searches_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io8WjqvRdjkk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import random\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from PyDictionary import PyDictionary\n",
        "import wikipedia\n",
        "\n",
        "dictionary = PyDictionary() # instantiate \n",
        "\n",
        "def get_wiki_definition(card):\n",
        "  # Get the summary of the Wikipedia page\n",
        "  summary = wikipedia.summary(card)\n",
        "  return summary\n",
        "\n",
        "# # Example usage:\n",
        "# card = \"John Smith\"\n",
        "# definition = get_definition(card)\n",
        "# print(definition)  # prints the summary of the Wikipedia page \"John Smith\"\n",
        "\n",
        "def get_definition(card):\n",
        "  # Check the type of the card\n",
        "  if is_slang_phrase(card):\n",
        "    # Get the definition of the slang phrase from Urban Dictionary\n",
        "    url = \"https://api.urbandictionary.com/v0/define?term=\" + card\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    definition = data['list'][0]['definition']\n",
        "  elif is_person(card):\n",
        "    # Get the summary of the person from Wikipedia\n",
        "    definition = get_wiki_definition(card[0]) # for a card title get the definition (summary) of the wikipedia page.\n",
        "  else:\n",
        "    # this is a word so use PyDictionary library.\n",
        "    definition = dictionary.meaning(str(card[0])) # this could have mult. meanings returned. \n",
        "    definition = definition[0] # the first meaning. \n",
        "\n",
        "def get_random_slang_phrase():\n",
        "  # Make a request to the Urban Dictionary API to get a random slang phrase\n",
        "  url = \"https://api.urbandictionary.com/v0/random\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  slang_phrase = data['list'][0]['word']\n",
        "  return slang_phrase\n",
        "\n",
        "def get_random_wiki_person():\n",
        "  # Make a request to Wikipedia API to get a random person\n",
        "  url = \"https://en.wikipedia.org/w/api.php?action=query&format=json&list=random&rnnamespace=0&rnlimit=1\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  person = data['query']['random'][0]['title']\n",
        "  return person\n",
        "\n",
        "def get_random_wiki_word():\n",
        "  # Make a request to Wikipedia API to get a random word\n",
        "  url = \"https://en.wikipedia.org/w/api.php?action=query&format=json&list=random&rnnamespace=0&rnlimit=1\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  word = data['query']['random'][0]['title']\n",
        "  return word\n",
        "\n",
        "def create_a_card_deck(num_cards=30):\n",
        "  # create 30 random cards with equal distributions of slang, people, and words.\n",
        "  # avoid profanity, or lude sexual topics.\n",
        "  \n",
        "  # Create a list of all the cards\n",
        "  cards = []\n",
        "\n",
        "  # Initialize a dictionary to keep track of the count of each type of card\n",
        "  counts = {\n",
        "    \"slang\": 0,\n",
        "    \"people\": 0,\n",
        "    \"words\": 0\n",
        "  }\n",
        "\n",
        "  # While there are fewer than 30 cards in the deck, generate a new card\n",
        "  while len(cards) < num_cards:\n",
        "    # Find the type of card with the lowest count\n",
        "    min_count_type = min(counts, key=counts.get)\n",
        "\n",
        "    # Generate a new card of that type\n",
        "    if min_count_type == \"slang\":\n",
        "      slang_phrase = get_random_slang_phrase()\n",
        "      new_card = (slang_phrase, \"slang\", get_definition(slang_phrase))\n",
        "    elif min_count_type == \"people\":\n",
        "      person = get_random_wiki_person()\n",
        "      new_card = (person, \"people\", get_definition(person))\n",
        "    else:\n",
        "      word = get_random_wiki_word()\n",
        "      new_card = (word, \"word\", get_definition(word))\n",
        "\n",
        "    # Add the new card to the list and update the count\n",
        "    cards.append(new_card)\n",
        "    counts[min_count_type] += 1 \n",
        "  return cards\n",
        "\n",
        "def is_slang_phrase(card):\n",
        "  if card[1] == 'slang':\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def is_person(card):\n",
        "  if card[1] == 'person':\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def filter_cards(cards):\n",
        "  # create a function to filter out inappropriate content\n",
        "  filtered_cards = []\n",
        "  for card in cards:\n",
        "\n",
        "    # check if the card contains profanity or sexual content\n",
        "    print(\"checking card: {}\".format(card))\n",
        "    definition = card[2] # definition\n",
        "    card_type = card[1] # i.e. 'slang'\n",
        "    word = card[0] # word \n",
        "    try:\n",
        "      if \"profanity\" in definition or \"sex\" in definition or \"lewd\" in definition:\n",
        "        continue\n",
        "      elif check_for_badwords():\n",
        "        continue\n",
        "      else:\n",
        "        filtered_cards.append(card)\n",
        "    except Exception as e:\n",
        "      print(f'error with finding the meaning in the soup object. soup comes from the urban dictionary page.')\n",
        "      print(e)\n",
        "  return filtered_cards\n",
        "\n",
        "# generate a deck of cards\n",
        "card_deck = create_a_card_deck()\n",
        "\n",
        "# filter the deck for inappropriate content\n",
        "filtered_deck = filter_cards(card_deck)\n",
        "\n",
        "# print the cards\n",
        "print('Generated {} cards'.format(len(filtered_deck)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tif9XZI5oRTR"
      },
      "outputs": [],
      "source": [
        "# print the filtered cards\n",
        "for card in filtered_deck:\n",
        "  print(card)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TcE6dW8-u3Ni"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "card = \"dope\"\n",
        "definition = get_definition(card)\n",
        "card_type = \"phrase\"\n",
        "card_preview = \n",
        "\n",
        "# (slang_phrase, \"slang\", get_definition(slang_phrase))\n",
        "    \n",
        "\n",
        "\n",
        "print(definition)  # prints the definition of the slang phrase \"dope\"\n",
        "\n",
        "card = \"John Smith\"\n",
        "definition = get_definition(card)\n",
        "print(definition)  # prints the summary of the person \"John Smith\"\n",
        "\n",
        "card = \"tree\"\n",
        "definition = get_definition(card)\n",
        "print(definition)  # prints the definition of the word \"tree\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
