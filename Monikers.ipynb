{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_wgZ8DsdcYz",
        "outputId": "51680fd1-495e-40fa-b5bc-38c2fdc650b5"
      },
      "outputs": [],
      "source": [
        "# !pip install requests\n",
        "# !pip install beautifulsoup4\n",
        "# !pip install wikipedia\n",
        "# !pip install PyDictionary\n",
        "# !pip install pytrends\n",
        "# !pip install urbandictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 10 trending searches.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>exploreQuery</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wordle</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Election results</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Betty White</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Queen Elizabeth</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bob Saget</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Ukraine</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mega Millions</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Powerball numbers</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Anne Heche</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Jeffrey Dahmer</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               title exploreQuery\n",
              "0             Wordle             \n",
              "1   Election results             \n",
              "2        Betty White             \n",
              "3    Queen Elizabeth             \n",
              "4          Bob Saget             \n",
              "5            Ukraine             \n",
              "6      Mega Millions             \n",
              "7  Powerball numbers             \n",
              "8         Anne Heche             \n",
              "9     Jeffrey Dahmer             "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pytrends\n",
        "from pytrends.request import TrendReq\n",
        "import wikipedia\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import PyDictionary\n",
        "from PyDictionary import PyDictionary\n",
        "#import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "dir(pytrends)\n",
        "pytrends = TrendReq(hl='en-US', tz=360) # create a pytrends object\n",
        "#pytrends.trending_searches(pn='united_states') # trending searches in real time for United States\n",
        "# create a dictionary of the top 1000 trending searches for this year in the United States (as of 2022)\n",
        "trending_searches_dict = pytrends.top_charts(\n",
        "    date='2022',    # specifies the time period as the last year\n",
        "    geo='US',            # specifies the country as the United States\n",
        ")\n",
        "real_time_trending_searches_dict = pytrends.trending_searches(pn='united_states') # trending searches in real time for United States\n",
        "\n",
        "\n",
        "\n",
        "# show how many trending searches there are\n",
        "print(f'Retrieved {len(trending_searches_dict)} trending searches.')\n",
        "# show the top 10 trending searches in real time for United States\n",
        "trending_searches_dict.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "bad_patterns = [r'sex*', r'porn*',r'fuck*',r'-ass*','ass','shit',r'damn*',r'ass|asse*',r'cock*',r'whor*',r'nigg*',r'slut*','blowjob',r'fagg*',r'boob|boob*', r'bitch*', r'bastard*',' ho |hoe',r'breast*|jugs', r'cunt*', r'puss*', r'dick*', 'naked', r'nud*', r'masterb*',r'mastu',r'nipple*',r'penis|penal|peni*','god','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','ejaculate','jew*','islamic','atheist',r'rapist*|rape*',r'pedo*','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','gay',r'tit*|titt*','god','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','islamic','atheist','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','yahwey','yeshua',r'israel*',r'sex*', r'porn*',r'fuck*',r'-ass*','ass','shit',r'damn*',r'ass|asse*',r'cock*',r'whor*',r'nigg*',r'slut*','blowjob',r'fagg*',r'boob|boob*', r'breast*|jugs', r'cunt*', r'puss*', r'dick*', 'naked', r'nud*', r'nipple*',r'penis|penal|peni*','god','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','islamic','atheist','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','gay',r'tit*|titt*', 'fellatio', 'fuck', 'nigger','lynch',r'erotic*','screw','lay','right wing','black man','black girl']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urbandictionary as ud\n",
        "import re\n",
        "# adding import alias recommended\n",
        "bad_patterns = [r'sex*', r'porn*',r'fuck*',r'-ass*','ass','shit',r'damn*',r'ass|asse*',r'cock*',r'whor*',r'nigg*',r'slut*','blowjob',r'fagg*',r'boob|boob*', r'bitch*', r'bastard*',' ho |hoe',r'breast*|jugs', r'cunt*', r'puss*', r'dick*', 'naked', r'nud*', r'masterb*',r'mastu',r'nipple*',r'penis|penal|peni*','god','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','ejaculate','jew*','islamic','atheist',r'rapist*|rape*',r'pedo*','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','gay',r'tit*|titt*','god','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','islamic','atheist','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','yahwey','yeshua',r'israel*',r'sex*', r'porn*',r'fuck*',r'-ass*','ass','shit',r'damn*',r'ass|asse*',r'cock*',r'whor*',r'nigg*',r'slut*','blowjob',r'fagg*',r'boob|boob*', r'breast*|jugs', r'cunt*', r'puss*', r'dick*', 'naked', r'nud*', r'nipple*',r'penis|penal|peni*','god','jesus','christ','bible','church','religion','pray','prayer','faith','lord','allah','muslim','islam','allah','islamic','atheist','atheism','atheists','atheist','atheists','christian','christianity','christians','christian','christians','gay',r'tit*|titt*', 'fellatio', 'fuck', 'nigger','lynch',r'erotic*','screw','lay','right wing','black man','black girl','nigga','racial']\n",
        "\n",
        "# to make the game more fun we can replace words with less inflamatory ones.\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "def check_for_badwords(definition, bad_patterns):\n",
        "    # if any of the buzzwords are found return a list of the matches, else an empty list\n",
        "    # include porter stemmer to match variations of the words in the bad_patterns list\n",
        "    from nltk.stem import PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in bad_patterns if len(stemmer.stem(word)) >= 4]\n",
        "    # only include the stemmed words which begin with the same letter as the original word (this is to avoid adding words that are not related to the original word)\n",
        "    stemmed_words = [word for word in stemmed_words if word[0] == word[0]]\n",
        "    bad_patterns.extend(stemmed_words)\n",
        "    # only include stemmed words that are not already in the list of words\n",
        "    bad_patterns = list(set(bad_patterns))\n",
        "    # create a list of regex patterns that match the words in the list\n",
        "    patterns = [re.compile(rf'\\b{word}\\b', re.IGNORECASE) for word in bad_patterns]\n",
        "    # return the list of regex patterns (regex patterns are used to match words, and are more efficient than using the 'in' operator)\n",
        "    \n",
        "    matches = [match.group(0) for match in re.finditer(r'\\b(?:' + '|'.join(bad_patterns) + r')\\b', definition, re.IGNORECASE)]\n",
        "    if len(matches)>0:\n",
        "        print(f'Found bad words in definition: {matches}')\n",
        "    return matches\n",
        "\n",
        "\n",
        "def check_for_good_patterns(definition, title):\n",
        "    # check both title and definition for good patterns\n",
        "    good_patterns = [r'\\b\\w+phobia\\.?\\b', r'\\bslang\\b', r'\\bacronymn\\b', r'\\bmeme\\b']\n",
        "\n",
        "\n",
        "    if any(re.match(r'\\b' + word + r'\\b', definition) for word in good_patterns):\n",
        "        print(f'Found a good pattern in the definition: {definition}')\n",
        "        return True\n",
        "    elif any(re.match(r'\\b' + word + r'\\b', title) for word in good_patterns):\n",
        "        print(f'Found a good pattern in the title: {title}')\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def remove_undesireable_sentences(definition, title, bad_patterns):\n",
        "    # convert the definition and title to lowercase\n",
        "    definition = definition.lower()\n",
        "    title = title.lower()\n",
        "\n",
        "    # split the definition into sentences\n",
        "    sentences = definition.split('.')\n",
        "\n",
        "    # split the title into words\n",
        "    title_words = title.split()\n",
        "\n",
        "    # initialize a list to store the acceptable sentences\n",
        "    acceptable_sentences = []\n",
        "\n",
        "    # iterate over the sentences in the definition\n",
        "    for sentence in sentences:\n",
        "        # split the sentence into words\n",
        "        sentence_words = sentence.split()\n",
        "\n",
        "        # check if any of the words in the sentence are also in the title\n",
        "        if not any(word in title_words for word in sentence_words):\n",
        "            # if not, add the sentence to the list of acceptable sentences\n",
        "            acceptable_sentences.append(sentence)\n",
        "\n",
        "    # remove sentences that are not in English\n",
        "    definition = re.sub(r'[^\\x00-\\x7f]',r'', '.'.join(acceptable_sentences))\n",
        "    # remove sentences that contain a regex match to any word in the buzzwords list\n",
        "    definition = re.sub(r'|'.join(map(re.escape, bad_patterns)), '', definition)\n",
        "    return definition\n",
        "\n",
        "\n",
        "def unpack_definitions(definition):\n",
        "    # remove the brackets and clean up the definitions\n",
        "    # with regex\n",
        "    definition = definition.replace(\"[\",\"\")\n",
        "    definition = definition.replace(\"]\",\"\")\n",
        "    definition = definition.replace(\"'\",\"\")\n",
        "    definition = definition.replace('\"',\"\")\n",
        "    definition = definition.replace(\"(\",\"\")\n",
        "    definition = definition.replace(\")\",\"\")\n",
        "\n",
        "    # remove double spaces\n",
        "    definition = definition.replace('  ',' ')\n",
        "\n",
        "    return definition\n",
        "\n",
        "# import wikipedia\n",
        "\n",
        "# def get_page_length(phrase):\n",
        "#   # search for pages on Wikipedia that match the given phrase\n",
        "#     try:\n",
        "#         pages = wikipedia.search(phrase)\n",
        "\n",
        "#         # retrieve the first page from the search results\n",
        "#         page = wikipedia.page(pages[0])\n",
        "#         print(f'Found the page for {page.title}', end='')\n",
        "#         if phrase.lower() != page.title.lower():\n",
        "#             print(' ... nevermind... not the right page.')\n",
        "#             return 0\n",
        "#         # return the length of the page\n",
        "#         return len(page.content)\n",
        "#     except Exception as e:\n",
        "#             return 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def prepper(words):\n",
        "#     # create a list of regex patterns that match the words in the list\n",
        "#     patterns = [re.compile(rf'\\b{word}\\b', re.IGNORECASE) for word in words]\n",
        "\n",
        "#     # add any variations of the words to the list of words by porter stemming\n",
        "#     from nltk.stem import PorterStemmer\n",
        "#     stemmer = PorterStemmer()\n",
        "#     stemmed_words = [stemmer.stem(word) for word in words if len(stemmer.stem(word)) >= 4]\n",
        "#     # only include the stemmed words which begin with the same letter as the original word (this is to avoid adding words that are not related to the original word)\n",
        "#     stemmed_words = [word for word in stemmed_words if word[0] == word[0]]\n",
        "#     words.extend(stemmed_words)\n",
        "#     # only include stemmed words that are not already in the list of words\n",
        "#     words = list(set(words))\n",
        "#     # create a list of regex patterns that match the words in the list\n",
        "#     patterns = [re.compile(rf'\\b{word}\\b', re.IGNORECASE) for word in words]\n",
        "#     # return the list of regex patterns (regex patterns are used to match words, and are more efficient than using the 'in' operator)\n",
        "#     return patterns\n",
        "\n",
        "\n",
        "\n",
        "# # create a function that takes a list of words and a list of regex patterns and returns a list of words that match the regex patterns\n",
        "# def match_words(words):\n",
        "#     words = words.split() # split the words string into a list of words\n",
        "#     try:\n",
        "#         patterns = prepper(words) # create the regex patterns\n",
        "#         matched_words = [] # initialize a list to store the matched words (words that match the regex patterns and indicate a bad word)\n",
        "#         for pattern in patterns: # iterate over the patterns (regex patterns)\n",
        "#             for word in words: # iterate over the words in the list of words.\n",
        "#                 if pattern.match(word): # if the word matches the regex pattern, add it to the list of matched words, and print a message to the console.\n",
        "#                     matched_words.append(word) # add the word to the list of matched words, if it matches the regex pattern\n",
        "#         return matched_words, patterns\n",
        "#     except Exception as e:\n",
        "#         print(e)\n",
        "#         return [],[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added yet: the word yet is a word that ca...\n",
            "Added shoot your shot: spit game....\n",
            "Added pisser: some where to go for a piss at...\n",
            "Added sobres: spanish word used in slang for...\n",
            "Added Tumblr: worlds first digital insane as...\n",
            "Added City Museum:  louis, missouri, opened in 19...\n",
            "Added otl: acronym standing for one true ...\n",
            "Found a good pattern in the title: rhabdophobia\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_36554/2001054441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# check if any element in the list is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_for_badwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbad_patterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_for_badwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbad_patterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# check if the phrase or definition contains any buzzwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m# include the element in the dictionary if it has a combined total of at least 100 upvotes and downvotes on Urban Dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       definition = remove_undesireable_sentences(\n",
            "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_36554/1211054009.py\u001b[0m in \u001b[0;36mcheck_for_badwords\u001b[0;34m(definition, bad_patterns)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mstemmed_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbad_patterns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# only include the stemmed words which begin with the same letter as the original word (this is to avoid adding words that are not related to the original word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mstemmed_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstemmed_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_36554/1211054009.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mstemmed_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbad_patterns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# only include the stemmed words which begin with the same letter as the original word (this is to avoid adding words that are not related to the original word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mstemmed_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstemmed_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mORIGINAL_ALGORITHM\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m             \u001b[0;31m# With this line, strings of length 1 or 2 don't go through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;31m# the stemming process, although no mention is made of this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# example: usage example,\n",
        "# upvotes: number of upvotes on Urban Dictionary,\n",
        "# downvotes: number of downvotes on Urban Dictionary\n",
        "import time\n",
        "wikitest = False # set to true to test the wikipedia page length\n",
        "# include a phrase if it has a combined total of at least 100 upvotes and downvotes on Urban Dictionary\n",
        "rand_dict = {}\n",
        "total_votes_thresh = 200 # min number of upvotes + downvotes\n",
        "upvotes_thresh = 100 # min number of upvotes\n",
        "downvotes_thresh = 10 # max number of downvotes\n",
        "desired_number_of_cards = 50\n",
        "rands = [] # the randoms\n",
        "\n",
        "while len(rand_dict) < desired_number_of_cards:\n",
        "  time.sleep(1)\n",
        "  rand = ud.random() # returns a list of 5 random phrases and definitions from Urban Dictionary\n",
        "  # append these to a master list\n",
        "  rands.extend(rand) # rands is a list of all the random phrases and definitions from Urban Dictionary\n",
        "\n",
        "  # iterate over the elements in the rand object\n",
        "  for element in rand:\n",
        "    # extract the relevant data from the element\n",
        "    phrase = element.word\n",
        "    definition = element.definition\n",
        "    usage_example = element.example\n",
        "    upvotes = element.upvotes\n",
        "    downvotes = element.downvotes\n",
        "    # define a list of boolean values\n",
        "    values = [upvotes + downvotes >= total_votes_thresh,\n",
        "              upvotes >= upvotes_thresh and downvotes <= downvotes_thresh,\n",
        "              check_for_good_patterns(definition, phrase)]\n",
        "    \n",
        "\n",
        "    # check if any element in the list is True\n",
        "    if any(values) and not check_for_badwords(definition, bad_patterns) \\\n",
        "      and not check_for_badwords(phrase, bad_patterns): # check if the phrase or definition contains any buzzwords\n",
        "    # include the element in the dictionary if it has a combined total of at least 100 upvotes and downvotes on Urban Dictionary\n",
        "      definition = remove_undesireable_sentences(\n",
        "        definition, phrase, bad_patterns) # remove sentences that contain buzzwords, are not in English, etc.\n",
        "      definition = unpack_definitions(definition) # remove brackets and clean up the definitions with regex\n",
        "\n",
        "      print(f'Added {phrase}: {definition[0:30]}...')\n",
        "      rand_dict[phrase] = definition\n",
        "\n",
        "# save the cards to a csv file\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_dict(rand_dict, orient='index')\n",
        "df.to_csv('cards.csv', header=False)\n",
        "\n",
        "# save the cards to a json file\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "try:\n",
        "  with open('cards_{}.json'.format(str(datetime.datetime.now())), 'w') as f:\n",
        "      json.dump(rand_dict, f)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "  with open('saved_cards.json', 'w') as f:\n",
        "      json.dump(rand_dict, f)\n",
        "\n",
        "  \n",
        "# Hippopotomonstrosesquippedaliophobia - fear of long words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#todo --- test this before using it\n",
        "def check_description_for_leaks(description, phrase):\n",
        "    # check if the description contains any of the words in the phrase, if so change the words to similar words, and return the new description using the same capitalization as the original description, and nltk to tokenize the words.\n",
        "    # tokenize the description and phrase\n",
        "    description_words = word_tokenize(description)\n",
        "    phrase_words = word_tokenize(phrase)\n",
        "\n",
        "    # loop through the phrase words and check if they are in the description\n",
        "    for i, word in enumerate(description_words):\n",
        "        if word in phrase_words:\n",
        "            # replace the word with a similar word\n",
        "            description_words[i] = 'XXXX'\n",
        "\n",
        "    # join the modified words back into a single string\n",
        "    modified_description = ' '.join(description_words)\n",
        "\n",
        "    # loop through the phrase words and modify the capitalization in the modified description\n",
        "    for word in phrase_words:\n",
        "        # find the index of the original word in the description\n",
        "        index = description.find(word)\n",
        "        # get the capitalization of the original word\n",
        "        capitalization = description[index:index+len(word)]\n",
        "        # modify the capitalization of the XXXX string\n",
        "        if capitalization.isupper():\n",
        "            modified_word = 'XXXX'.upper()\n",
        "        elif capitalization.istitle():\n",
        "            modified_word = 'XXXX'.title()\n",
        "        else:\n",
        "            modified_word = 'XXXX'\n",
        "        # replace the original word with the modified XXXX string\n",
        "        modified_description = modified_description.replace(word, modified_word, 1)\n",
        "\n",
        "    return modified_description\n",
        "\n",
        "\n",
        "def check_for_badwords(definition, bad_patterns):\n",
        "    # if any of the buzzwords are found return a list of the matches, else an empty list\n",
        "    # include porter stemmer to match variations of the words in the bad_patterns list\n",
        "    from nltk.stem import PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in bad_patterns if len(stemmer.stem(word)) >= 4]\n",
        "    # only include the stemmed words which begin with the same letter as the original word (this is to avoid adding words that are not related to the original word)\n",
        "    stemmed_words = [word for word in stemmed_words if word[0] == word[0]]\n",
        "    bad_patterns.extend(stemmed_words)\n",
        "    # only include stemmed words that are not already in the list of words\n",
        "    bad_patterns = list(set(bad_patterns))\n",
        "    # create a list of regex patterns that match the words in the list\n",
        "    patterns = [re.compile(rf'\\b{word}\\b', re.IGNORECASE) for word in bad_patterns]\n",
        "    # return the list of regex patterns (regex patterns are used to match words, and are more efficient than using the 'in' operator)\n",
        "    \n",
        "    matches = [match.group(0) for match in re.finditer(r'\\b(?:' + '|'.join(bad_patterns) + r')\\b', definition, re.IGNORECASE)]\n",
        "    print('Buzzwords found:', matches)\n",
        "    return matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['nigga', 'right wing', 'sex']"
            ]
          },
          "execution_count": 280,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing the patterns\n",
        "test = \"a racial slur used by nigga ignorant hypocritical right wing retards. if you think about it, we are all illegals, the ancestors came sex into north america the same way that mexicans are entering into our country and becoming citizens now. so if you have a problem with this, then you are a racist hypocrite!\"\n",
        "\n",
        "check_for_badwords(test, bad_patterns=bad_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# initialize the sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def has_sexual_comment(phrase, bad_patterns):\n",
        "    # analyze the sentiment of the phrase\n",
        "    scores = analyzer.polarity_scores(phrase)\n",
        "    # check if the compound score is greater than or equal to 0.5, which indicates a positive sentiment\n",
        "    if scores['compound'] <= 0.5:\n",
        "        print(f' * Found a negative sentiment in the phrase: {phrase}')\n",
        "        print(f'Score: {scores}')\n",
        "    \n",
        "        # if the phrase also contains a buzzword, return True\n",
        "        if any(re.match(pattern, phrase) for pattern in bad_patterns):\n",
        "            print(f' ** Also, Found a bad pattern in the phrase: {phrase}')\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_phrase = \"a single or collection of females that you shamefully solicit for sexual acts but what be disparaged if anyone knew that you were hooking up with them.\"\n",
        "\n",
        "has_sex = has_sexual_comment(test_phrase, bad_patterns)\n",
        "\n",
        "print('Testing for sex in the phrase, {}: {}'.format(test_phrase,has_sex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_phrase =  \"another phrased coined by david alan weinshel wheeler 02 bu 06. simplifies the stress of saying how and come separately. also eliminates the need to hit hte spacebar when typing.\"\n",
        "has_sex = has_sexual_comment(test_phrase, bad_patterns)\n",
        "\n",
        "print('Testing for sex in the phrase, {}: {}'.format(test_phrase,has_sex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_time_trending_searches_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io8WjqvRdjkk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import random\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from PyDictionary import PyDictionary\n",
        "import wikipedia\n",
        "\n",
        "dictionary = PyDictionary() # instantiate \n",
        "\n",
        "def get_wiki_definition(card):\n",
        "  # Get the summary of the Wikipedia page\n",
        "  summary = wikipedia.summary(card)\n",
        "  return summary\n",
        "\n",
        "# # Example usage:\n",
        "# card = \"John Smith\"\n",
        "# definition = get_definition(card)\n",
        "# print(definition)  # prints the summary of the Wikipedia page \"John Smith\"\n",
        "\n",
        "def get_definition(card):\n",
        "  # Check the type of the card\n",
        "  if is_slang_phrase(card):\n",
        "    # Get the definition of the slang phrase from Urban Dictionary\n",
        "    url = \"https://api.urbandictionary.com/v0/define?term=\" + card\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    definition = data['list'][0]['definition']\n",
        "  elif is_person(card):\n",
        "    # Get the summary of the person from Wikipedia\n",
        "    definition = get_wiki_definition(card[0]) # for a card title get the definition (summary) of the wikipedia page.\n",
        "  else:\n",
        "    # this is a word so use PyDictionary library.\n",
        "    definition = dictionary.meaning(str(card[0])) # this could have mult. meanings returned. \n",
        "    definition = definition[0] # the first meaning. \n",
        "\n",
        "def get_random_slang_phrase():\n",
        "  # Make a request to the Urban Dictionary API to get a random slang phrase\n",
        "  url = \"https://api.urbandictionary.com/v0/random\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  slang_phrase = data['list'][0]['word']\n",
        "  return slang_phrase\n",
        "\n",
        "def get_random_wiki_person():\n",
        "  # Make a request to Wikipedia API to get a random person\n",
        "  url = \"https://en.wikipedia.org/w/api.php?action=query&format=json&list=random&rnnamespace=0&rnlimit=1\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  person = data['query']['random'][0]['title']\n",
        "  return person\n",
        "\n",
        "def get_random_wiki_word():\n",
        "  # Make a request to Wikipedia API to get a random word\n",
        "  url = \"https://en.wikipedia.org/w/api.php?action=query&format=json&list=random&rnnamespace=0&rnlimit=1\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  word = data['query']['random'][0]['title']\n",
        "  return word\n",
        "\n",
        "def create_a_card_deck(num_cards=30):\n",
        "  # create 30 random cards with equal distributions of slang, people, and words.\n",
        "  # avoid profanity, or lude sexual topics.\n",
        "  \n",
        "  # Create a list of all the cards\n",
        "  cards = []\n",
        "\n",
        "  # Initialize a dictionary to keep track of the count of each type of card\n",
        "  counts = {\n",
        "    \"slang\": 0,\n",
        "    \"people\": 0,\n",
        "    \"words\": 0\n",
        "  }\n",
        "\n",
        "  # While there are fewer than 30 cards in the deck, generate a new card\n",
        "  while len(cards) < num_cards:\n",
        "    # Find the type of card with the lowest count\n",
        "    min_count_type = min(counts, key=counts.get)\n",
        "\n",
        "    # Generate a new card of that type\n",
        "    if min_count_type == \"slang\":\n",
        "      slang_phrase = get_random_slang_phrase()\n",
        "      new_card = (slang_phrase, \"slang\", get_definition(slang_phrase))\n",
        "    elif min_count_type == \"people\":\n",
        "      person = get_random_wiki_person()\n",
        "      new_card = (person, \"people\", get_definition(person))\n",
        "    else:\n",
        "      word = get_random_wiki_word()\n",
        "      new_card = (word, \"word\", get_definition(word))\n",
        "\n",
        "    # Add the new card to the list and update the count\n",
        "    cards.append(new_card)\n",
        "    counts[min_count_type] += 1 \n",
        "  return cards\n",
        "\n",
        "def is_slang_phrase(card):\n",
        "  if card[1] == 'slang':\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def is_person(card):\n",
        "  if card[1] == 'person':\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def filter_cards(cards):\n",
        "  # create a function to filter out inappropriate content\n",
        "  filtered_cards = []\n",
        "  for card in cards:\n",
        "\n",
        "    # check if the card contains profanity or sexual content\n",
        "    print(\"checking card: {}\".format(card))\n",
        "    definition = card[2] # definition\n",
        "    card_type = card[1] # i.e. 'slang'\n",
        "    word = card[0] # word \n",
        "    try:\n",
        "      if \"profanity\" in definition or \"sex\" in definition or \"lewd\" in definition:\n",
        "        continue\n",
        "      elif check_for_badwords():\n",
        "        continue\n",
        "      else:\n",
        "        filtered_cards.append(card)\n",
        "    except Exception as e:\n",
        "      print(f'error with finding the meaning in the soup object. soup comes from the urban dictionary page.')\n",
        "      print(e)\n",
        "  return filtered_cards\n",
        "\n",
        "# generate a deck of cards\n",
        "card_deck = create_a_card_deck()\n",
        "\n",
        "# filter the deck for inappropriate content\n",
        "filtered_deck = filter_cards(card_deck)\n",
        "\n",
        "# print the cards\n",
        "print('Generated {} cards'.format(len(filtered_deck)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tif9XZI5oRTR"
      },
      "outputs": [],
      "source": [
        "# print the filtered cards\n",
        "for card in filtered_deck:\n",
        "  print(card)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TcE6dW8-u3Ni"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "card = \"dope\"\n",
        "definition = get_definition(card)\n",
        "card_type = \"phrase\"\n",
        "card_preview = \n",
        "\n",
        "# (slang_phrase, \"slang\", get_definition(slang_phrase))\n",
        "    \n",
        "\n",
        "\n",
        "print(definition)  # prints the definition of the slang phrase \"dope\"\n",
        "\n",
        "card = \"John Smith\"\n",
        "definition = get_definition(card)\n",
        "print(definition)  # prints the summary of the person \"John Smith\"\n",
        "\n",
        "card = \"tree\"\n",
        "definition = get_definition(card)\n",
        "print(definition)  # prints the definition of the word \"tree\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
