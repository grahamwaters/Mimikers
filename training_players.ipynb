{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on 1000 actual pages randomly from wikipedia (create the training set)\n",
    "import re\n",
    "import requests\n",
    "import wikipedia\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "def gather_page(category, page_dict):\n",
    "    # Get a random page from wikipedia in the category\n",
    "    page = wikipedia.random(category=category)\n",
    "    # Get the page content\n",
    "    content = wikipedia.page(page)\n",
    "    text_content = content.content\n",
    "    related = content.links\n",
    "    # Add the content to the dictionary\n",
    "    page_dict[\"title\"].append(page) # append the title\n",
    "    page_dict[\"text\"].append(text_content) # append the text content\n",
    "    page_dict[\"related\"].append(related) # supplied at training time\n",
    "    page_dict[\"category\"].append(category) # supplied at training time\n",
    "    return page_dict # return the page dictionary\n",
    "\n",
    "def gather_training_data(pages_count, category):\n",
    "    # Gather the pages from wikipedia\n",
    "    training_set = [] # list of dictionaries with title, text, and category\n",
    "    page_dict = {\"title\": [], \"text\": [], \"related\":[], \"category\": []}\n",
    "    for i in tqdm(range(pages_count)):\n",
    "        page_dict = gather_page(category, page_dict) # gather the page (1) and add it to the training set (2)\n",
    "        print(\"Gathered page \" + str(i+1) + \" of \" + str(pages_count) + \" for category \" + category + \".\")\n",
    "        print(\"Page title: \" + page_dict[\"title\"][-1]) # print the title of the page\n",
    "        training_set.append(page_dict) # add the page to the training set\n",
    "        # add the training set to the training set\n",
    "\n",
    "    # save the data to a csv file for later use\n",
    "    df = pd.DataFrame(page_dict) # convert to a dataframe for easy saving\n",
    "    filename = \"./data/training_data_\" + category + \".csv\"\n",
    "    df.to_csv(filename, index=False) # save the data to a csv file\n",
    "    return training_set\n",
    "\n",
    "\n",
    "categories = [\"Living people\", \"Popular culture\", \"Internet memes\", \"Viral videos\", \"Social media\",\"Social_phenomena\",\"Phenomena\",\"Pseudohistory\",\"Hoaxes\",\"Hoaxes_in_science\",\"Written_fiction_presented_as_fact\",\"Memetics\",\"TikTokers\",\"Social_media\",\"Fake_news\",\"Word_play\",\"IOS_games\",\"IOS_software\",\"Works_involved_in_plagiarism_controversies\",\"YouTube_controversies\",\"2000s_YouTube_series\",\"2010s_YouTube_series\",\"Psychotherapy\",\"Psychology\",\"Human medicine\",\"Interpersonal_relationships\",\"Psychological_therapy\",\"Human_behavior\",\"Love\",\"Philosophers_of_ethics_and_morality\",\"United_States_Supreme_Court_cases\",\"Political_party_founders\",\"Classics_educators\",\"Fictional_inventors\",\"Whistleblowing\",\"News_leaks\",\"WikiLeaks\",\"Popular_music\",\"Puzzles\",\"Fiction_about_personifications_of_death\",\"Bogeymen\",\"Comedians\",\"Authors\",\"Artists\",\"Literary_concepts\",\"Historical_eras\",\"Viral_videos\",\"Internet_memes\",\"Theorems\",\"21st-century_male_actors\",\"21st-century_female_actors\",\"Fables\",\"American_Internet_celebrities\",\"Legends\",\"Mythology\",\"Rules_of_thumb\",\"Adages\",\"Fallacies\",\"Mountains\",\"Lakes\",\"Oceans\",\"Sea_Monsters\",\"fairy_tales\",\"1800s\",\"Tall_tales\",\"Urban_legends\",\"Superstitions\",\"Western_culture\",\"Quotations_from_film\",\"Quotations_from_music\",\"Quotations_from_literature\",\"Quotations_from_television\",\"Quotations_from_video_games\",\"English_phrases\", \"English_proverbs\", \"English-language_slang\", \"English_slang\",\"Universal_Pictures_films\", \"Paramount_Pictures_films\",\"Human Anatomy\",\"Human Physiology\",\"Human Behavior\",\"Melons\",\"Fruit\",\"Vegetables\",\"Flowers\",\"Trees\",\"Dinosaurs_in_popular_culture\",\"Extraterrestrial_life_in_popular_culture\",'English-language_idioms','Catchphrases','British_English_idioms','Living_people',\"Philosophers_of_ethics_and_morality\",\"United_States_Supreme_Court_cases\",\"Political_party_founders\",\"Classics_educators\",\"Social_phenomena\",\"Phenomena\",\"Pseudohistory\",\"Hoaxes\",\"Hoaxes_in_science\",\"Written_fiction_presented_as_fact\",\"Viral_videos\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time # for rate limiting\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Replace CATEGORY with the name of the category you want to search\n",
    "CATEGORY = \"Living people\"\n",
    "\n",
    "def request_pages(CATEGORY):\n",
    "\n",
    "    # Make a request to the Wikipedia API to search for pages in the specified category\n",
    "    response = requests.get(\"https://en.wikipedia.org/w/api.php\", params={\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": f\"Category:{CATEGORY}\",\n",
    "        \"cmlimit\": \"max\" # Set the maximum number of pages to return\n",
    "    })\n",
    "\n",
    "    page_dict = {\"title\": [], \"text\": [], \"related\":[], \"category\": []}\n",
    "    \n",
    "    # Get the list of pages from the response\n",
    "    pages = response.json()[\"query\"][\"categorymembers\"]\n",
    "\n",
    "    # get the content of each page\n",
    "    for page in tqdm(pages):\n",
    "        try:\n",
    "            # Get the title of the page\n",
    "            title = page[\"title\"]\n",
    "            # Get the content of the page\n",
    "            page = wikipedia.page(title)\n",
    "            # Add the page to the page dictionary\n",
    "            page_dict[\"title\"].append(title)\n",
    "            page_dict[\"text\"].append(page.content)\n",
    "            page_dict[\"related\"].append(page.links) # supplied at training time\n",
    "            page_dict[\"category\"].append(CATEGORY)\n",
    "            # every ten pages, save the data to a csv file\n",
    "            if len(page_dict[\"title\"]) % 10 == 0:\n",
    "                # convert the dict to a dataframe\n",
    "                df = pd.DataFrame.from_dict(page_dict, orient='index', columns=['summary'])\n",
    "                # save the dataframe to a csv file\n",
    "                df.to_csv(\"./data/summary_data_\" + CATEGORY + \".csv\")\n",
    "            time.sleep(0.75) # rate limit the requests\n",
    "        except:\n",
    "            pass\n",
    "    return page_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 42/500 [00:50<06:28,  1.18it/s]/opt/anaconda3/envs/groupme/lib/python3.9/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/anaconda3/envs/groupme/lib/python3.9/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      " 11%|█         | 56/500 [01:07<09:49,  1.33s/it]/opt/anaconda3/envs/groupme/lib/python3.9/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/anaconda3/envs/groupme/lib/python3.9/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      " 16%|█▌        | 80/500 [01:40<10:33,  1.51s/it]"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "page_dict = request_pages(CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('groupme')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28dd76f97a2595215b3511d9563b8125e93469ee739d17a6b25584482d270cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
